## Inpainting Stage ì¢…í•© ë¶„ì„

**ëª©í‘œ:** ë™ì  ê°ì²´ê°€ ì œê±°ëœ ì •ì  ë°°ê²½ ì´ë¯¸ì§€ ìƒì„±

---

## ğŸ“‹ ë‘ ê°€ì§€ Approach ê°œìš”

### Approach 1: COLMAP-based Scene Reconstruction
**ì „ëµ:** 3D ì¬êµ¬ì„± ê¸°ë°˜ ê³µê°„ì  ì¼ê´€ì„± ìš°ì„   
**í•™ìŠµ ëª¨ë¸:** âŒ ì—†ìŒ (ì „í†µì  ì»´í“¨í„° ë¹„ì „ ê¸°ë²•)

### Approach 2: Sequential Multi-Stage Pipeline
**ì „ëµ:** ì‹œê³„ì—´ ì •ë³´ í™œìš© + AI ìƒì„± ê¸°ë°˜ ì ì§„ì  ë³µì›  
**í•™ìŠµ ëª¨ë¸:** âœ… Step 3ì—ì„œ ì‚¬ìš© (Stable Diffusion + ControlNet + LoRA)

**ê³µí†µ ìµœì¢… Output:**
```
final_inpainted/
â””â”€â”€ *.jpg  # ë™ì  ê°ì²´ê°€ ì œê±°ëœ ê¹¨ë—í•œ ë°°ê²½ ì´ë¯¸ì§€
```

---

## ğŸ”„ Approach 1: COLMAP-based Scene Reconstruction

### âœ¨ í•µì‹¬ ì•„ì´ë””ì–´
**ì •ì  ì˜ì—­ë§Œìœ¼ë¡œ 3D ì¬êµ¬ì„± â†’ Novel View Synthesisë¡œ êµ¬ë© ì±„ìš°ê¸°**

### ğŸ“¥ Input
| í•­ëª© | í˜•ì‹ | ì¶œì²˜ | ì„¤ëª… |
|-----|------|------|------|
| Images | `images/*.jpg` | Parsing | ì›ë³¸ ì´ë¯¸ì§€ |
| Masks | `masks/{cam}/*.png` | Preprocessing | ë™ì  ê°ì²´ ë§ˆìŠ¤í¬ (0=ë™ì , 255=ì •ì ) |
| Poses | `poses/*.json` | Parsing | ì¹´ë©”ë¼ í¬ì¦ˆ (ì´ˆê¸°ê°’) |

### âš™ï¸ Process

#### **Step 1: Feature Extraction (íŠ¹ì§•ì  ì¶”ì¶œ)**

**ëª©ì :** ì •ì  ì˜ì—­ì—ì„œë§Œ SIFT íŠ¹ì§•ì  ì¶”ì¶œ

**Process:**
```bash
colmap feature_extractor \
    --database_path database.db \
    --image_path images/ \
    --ImageReader.mask_path masks/ \  # ë§ˆìŠ¤í¬ ì ìš©
    --ImageReader.camera_model OPENCV \
    --SiftExtraction.max_num_features 8192
```

**í•µì‹¬ ë¡œì§:**
- ë§ˆìŠ¤í¬ ì ìš©: `mask == 255` ì˜ì—­ì—ì„œë§Œ íŠ¹ì§•ì  ì¶”ì¶œ
- ë™ì  ê°ì²´ëŠ” ì™„ì „íˆ ë¬´ì‹œ â†’ ì •ì  ë°°ê²½ë§Œìœ¼ë¡œ ì¬êµ¬ì„±

**Output:**
- `database.db`: COLMAP ë°ì´í„°ë² ì´ìŠ¤ (íŠ¹ì§•ì , ë””ìŠ¤í¬ë¦½í„° ì €ì¥)

---

#### **Step 2: Feature Matching (íŠ¹ì§•ì  ë§¤ì¹­)**

**ëª©ì :** í”„ë ˆì„ ê°„ ëŒ€ì‘ì  ì°¾ê¸°

**Process:**
```bash
colmap sequential_matcher \
    --database_path database.db \
    --SequentialMatching.overlap 10 \  # ì „í›„ 10 í”„ë ˆì„
    --SequentialMatching.loop_detection 1
```

**í•µì‹¬ ë¡œì§:**
- Sequential matching: ììœ¨ì£¼í–‰ ì‹œí€€ìŠ¤ì— ìµœì í™”
- Loop detection: ë¹„ìŠ·í•œ ì¥ë©´ ì¬ë°©ë¬¸ ê°ì§€

**Output:**
- `database.db` (ì—…ë°ì´íŠ¸): ë§¤ì¹­ ê²°ê³¼ ì¶”ê°€

---

#### **Step 3: Structure from Motion (SfM, 3D ì¬êµ¬ì„±)**

**ëª©ì :** ì¹´ë©”ë¼ í¬ì¦ˆ ì¶”ì • ë° Sparse 3D í¬ì¸íŠ¸ ìƒì„±

**Process:**
```bash
colmap mapper \
    --database_path database.db \
    --image_path images/ \
    --output_path sparse/
```

**í•µì‹¬ ë¡œì§:**
- Incremental SfM:
  1. ì´ˆê¸° ì´ë¯¸ì§€ ìŒ ì„ íƒ
  2. Essential matrix ë¶„í•´ â†’ Pose ì¶”ì •
  3. Triangulation â†’ 3D í¬ì¸íŠ¸ ìƒì„±
  4. Bundle Adjustment â†’ ì „ì—­ ìµœì í™”
  5. ìƒˆ ì´ë¯¸ì§€ ì¶”ê°€ ë°˜ë³µ

**Output:**
```
sparse/0/
â”œâ”€â”€ cameras.bin       # ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„°
â”œâ”€â”€ images.bin        # ì¹´ë©”ë¼ í¬ì¦ˆ
â””â”€â”€ points3D.bin      # 3D í¬ì¸íŠ¸ í´ë¼ìš°ë“œ (ì •ì  ë°°ê²½ë§Œ)
```

**íŠ¹ì§•:**
- ë™ì  ê°ì²´ê°€ ì œê±°ëœ ìˆœìˆ˜ ë°°ê²½ 3D ëª¨ë¸
- Multi-view geometric consistency ë³´ì¥

---

#### **Step 4: Dense Reconstruction (MVS, ë°€ì§‘ ì¬êµ¬ì„±)**

**ëª©ì :** Sparse 3Dë¥¼ Dense Depth Mapìœ¼ë¡œ í™•ì¥

**Process:**
```bash
# 1. Image Undistortion
colmap image_undistorter \
    --input_path sparse/0 \
    --output_path dense/

# 2. Patch Match Stereo
colmap patch_match_stereo \
    --workspace_path dense/

# 3. Stereo Fusion
colmap stereo_fusion \
    --workspace_path dense/ \
    --output_path dense/fused.ply
```

**í•µì‹¬ ë¡œì§:**
1. **Undistortion**: ì™œê³¡ ì œê±°
2. **Patch Match Stereo**: 
   - ê° í”½ì…€ì˜ depth & normal ì¶”ì •
   - Multi-view photo consistency ìµœëŒ€í™”
3. **Fusion**: 
   - ì—¬ëŸ¬ ë·°ì˜ depthë¥¼ í•˜ë‚˜ë¡œ ìœµí•©
   - Outlier filtering

**Output:**
```
dense/
â”œâ”€â”€ images/              # Undistorted images
â”œâ”€â”€ stereo/
â”‚   â”œâ”€â”€ depth_maps/      # ê° ë·°ì˜ depth map (.bin)
â”‚   â”œâ”€â”€ normal_maps/     # Normal map
â”‚   â””â”€â”€ consistency_graphs/
â””â”€â”€ fused.ply            # Dense 3D point cloud (ì •ì  ë°°ê²½)
```

---

#### **Step 5: Hole Filling (êµ¬ë© ì±„ìš°ê¸°)**

**ëª©ì :** ì¬êµ¬ì„±ëœ 3Dë¥¼ ì›ë˜ ë·°ì— ë Œë”ë§í•˜ì—¬ ë™ì  ê°ì²´ ì˜ì—­ ì±„ìš°ê¸°

**í•µì‹¬ ë¡œì§:**
```python
for each frame:
    # 1. Load COLMAP depth map
    depth_map = read_colmap_depth(f"dense/stereo/depth_maps/{frame}.bin")
    
    # 2. Load original image & mask
    image = cv2.imread(f"images/{frame}.jpg")
    mask = cv2.imread(f"masks/{frame}.png")
    hole_mask = (mask == 0)  # ë™ì  ê°ì²´ ì˜ì—­
    
    # 3. Inpaint using depth guidance
    # Option A: Depth-based priority inpainting
    result = depth_guided_inpaint(image, hole_mask, depth_map)
    
    # Option B: Multi-view synthesis
    # ì´ì›ƒ ë·°ë“¤ì˜ ì •ë³´ë¥¼ í˜„ì¬ ë·°ì— íˆ¬ì˜
    result = multi_view_synthesis(image, hole_mask, depth_map, neighbor_views)
    
    # 4. Save
    cv2.imwrite(f"final_inpainted/{frame}.jpg", result)
```

**ë°©ë²•ë¡ :**

**A. Depth-guided Inpainting:**
- ê¹Šì´ ì •ë³´ë¡œ ìš°ì„ ìˆœìœ„ ì„¤ì •
- ê°€ê¹Œìš´ ë°°ê²½ë¶€í„° ì±„ì›Œë‚˜ê°
- OpenCV inpaint + depth priority queue

**B. Multi-view Synthesis:**
- ì´ì›ƒ í”„ë ˆì„ì˜ 3D í¬ì¸íŠ¸ë¥¼ í˜„ì¬ ë·°ì— íˆ¬ì˜
- ê°€ë ¤ì§„(occluded) ë°°ê²½ì„ ë‹¤ë¥¸ ë·°ì—ì„œ ë³µì›
- Weighted blending (confidence based)

**Output:**
- `final_inpainted/*.jpg`: ìµœì¢… ê²°ê³¼ (ì„ì‹œ, Step 6 ì „)

---

#### **Step 6: Post-processing (í›„ì²˜ë¦¬)**

**ëª©ì :** ì‹œê°„ì  ì¼ê´€ì„± ë° í’ˆì§ˆ í–¥ìƒ

**Process:**
```python
# 1. Temporal Smoothing
for i, frame in enumerate(frames):
    # ì´ì›ƒ í”„ë ˆì„ê³¼ ë¸”ë Œë”©
    neighbors = frames[i-1:i+2]  # Â±1 í”„ë ˆì„
    smoothed = weighted_average(frame, neighbors, weights=[0.2, 0.8, 0.2])
    
# 2. Texture Noise (ì„ íƒì )
# ë„ˆë¬´ ë§¤ëˆí•œ ì˜ì—­ì— ë¯¸ì„¸í•œ ë…¸ì´ì¦ˆ ì¶”ê°€
noise = np.random.normal(0, 3, smoothed.shape)
final = smoothed + noise * texture_mask

# 3. Seam Blending
# ì¸í˜ì¸íŒ… ê²½ê³„ë¥¼ ë¶€ë“œëŸ½ê²Œ
final = poisson_blending(final, original, hole_mask)
```

**Output:**
- `final_inpainted/*.jpg`: ìµœì¢… ì™„ì„± ê²°ê³¼

---

### ğŸ“¤ Final Output

```
final_inpainted/
â”œâ”€â”€ frame_000.jpg
â”œâ”€â”€ frame_001.jpg
â””â”€â”€ ...
```

**íŠ¹ì§•:**
- 3D ê¸°í•˜í•™ì  ì¼ê´€ì„± ë³´ì¥
- Multi-view consistency ìë™ ìœ ì§€
- ë°°ê²½ êµ¬ì¡°ê°€ ë³µì¡í•´ë„ ê°•ê±´

---

### ğŸ¯ ì¥ë‹¨ì 

#### âœ… ì¥ì 
1. **Multi-view Consistency**: 3D ì¬êµ¬ì„±ìœ¼ë¡œ ë·° ê°„ ì¼ê´€ì„± ìë™ ë³´ì¥
2. **Geometric Accuracy**: ì •í™•í•œ ë°°ê²½ ê¸°í•˜í•™ ë³µì›
3. **Robustness**: ë³µì¡í•œ ë°°ê²½ êµ¬ì¡°ì—ë„ ê°•ê±´
4. **No Learning Required**: í•™ìŠµ ë°ì´í„° ë¶ˆí•„ìš”

#### âš ï¸ ë‹¨ì 
1. **ê³„ì‚° ë¹„ìš©**: SfM + MVSëŠ” ë§¤ìš° ëŠë¦¼ (~ìˆ˜ ì‹œê°„)
2. **í…ìŠ¤ì²˜ í•œê³„**: Texture-less ì˜ì—­ì—ì„œ ì‹¤íŒ¨ ê°€ëŠ¥
3. **ì™„ì „ì„± ë¬¸ì œ**: ì¬êµ¬ì„± ì‹¤íŒ¨ ì‹œ êµ¬ë© ë‚¨ìŒ
4. **COLMAP ì˜ì¡´ì„±**: ì™¸ë¶€ ë„êµ¬ í•„ìˆ˜

---

### ğŸ“Š ì„±ëŠ¥ ì§€í‘œ

| í•­ëª© | ê°’ | ì„¤ëª… |
|-----|---|------|
| **ì²˜ë¦¬ ì†ë„** | ~1-5 ì‹œê°„ | 100 í”„ë ˆì„ ê¸°ì¤€ (GPU) |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©** | ~8-16GB | Dense MVS ë‹¨ê³„ |
| **3D í¬ì¸íŠ¸ ìˆ˜** | ~1M-10M | Sparse ë‹¨ê³„ |
| **Depth ì •í™•ë„** | ~cm ë‹¨ìœ„ | ì •ì  ì˜ì—­ ê¸°ì¤€ |
| **ì„±ê³µë¥ ** | ~80-90% | ì¬êµ¬ì„± ì„±ê³µë¥  |

---

## ğŸ”„ Approach 2: Sequential Multi-Stage Pipeline

### âœ¨ í•µì‹¬ ì•„ì´ë””ì–´
**ì‹œê³„ì—´ ëˆ„ì  â†’ ê¸°í•˜í•™ì  ê°€ì´ë“œ â†’ AI ìƒì„±ì˜ 3ë‹¨ê³„ ì ì§„ì  ë³µì›**

### ğŸ“¥ Input
| í•­ëª© | í˜•ì‹ | ì¶œì²˜ | ì„¤ëª… |
|-----|------|------|------|
| Images | `images/*.jpg` | Parsing | ì›ë³¸ ì´ë¯¸ì§€ |
| Masks | `masks/{cam}/*.png` | Preprocessing | ë™ì  ê°ì²´ ë§ˆìŠ¤í¬ |
| Poses | `poses/*.json` | Parsing | ì¹´ë©”ë¼ í¬ì¦ˆ |
| Depth Maps | `depth_maps/{cam}/*.png` | Preprocessing | LiDAR ê¹Šì´ ë§µ (ì„ íƒ) |

### âš™ï¸ Process

---

### **Step 1: Temporal Accumulation (ì‹œê³„ì—´ ëˆ„ì )**
(`step1_temporal_accumulation.py`)

**í•™ìŠµ ëª¨ë¸:** âŒ ì—†ìŒ (ê¸°í•˜í•™ì  3D íˆ¬ì˜ ê¸°ë°˜)

#### ëª©ì 
**ì—¬ëŸ¬ í”„ë ˆì„ì˜ ì •ì  ë°°ê²½ì„ 3Dë¡œ ëˆ„ì í•˜ì—¬ ë™ì  ê°ì²´ ë’¤ì˜ ë°°ê²½ ë³µì›**

#### í•µì‹¬ ì•„ì´ë””ì–´
```
ë™ì  ê°ì²´ê°€ ì´ë™ â†’ ì´ì „/ì´í›„ í”„ë ˆì„ì—ì„œëŠ” ê·¸ ìœ„ì¹˜ê°€ ë°°ê²½
â†’ ì‹œê³„ì—´ ì •ë³´ë¥¼ 3Dë¡œ ìœµí•©í•˜ë©´ ì™„ì „í•œ ë°°ê²½ íšë“
```

#### Process

**1.1 Forward Pass (ìˆœë°©í–¥ ëˆ„ì )**
```python
# ê° í”„ë ˆì„ì„ ê¸°ì¤€ìœ¼ë¡œ Forwardë¡œ ëˆ„ì 
for ref_frame_idx in range(len(frames)):
    # 3D í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ì´ˆê¸°í™”
    accumulated_points = PointCloud()
    
    # ì´í›„ í”„ë ˆì„ë“¤ì„ ìƒ˜í”Œë§
    for src_frame_idx in range(ref_frame_idx + 1, len(frames), sample_interval):
        # 1. Source í”„ë ˆì„ì˜ ì •ì  ì˜ì—­ë§Œ ì„ íƒ
        src_image = load_image(src_frame_idx)
        src_mask = load_mask(src_frame_idx)
        static_pixels = src_image[src_mask == 255]
        
        # 2. ê¹Šì´ ì¶”ì • (LiDAR or Pseudo-depth)
        depth = load_depth(src_frame_idx)
        
        # 3. 2D â†’ 3D Unprojection (ì—­íˆ¬ì˜)
        points_3d_src = unproject(static_pixels, depth, K_src, T_src_to_world)
        
        # 4. 3D í¬ì¸íŠ¸ ëˆ„ì 
        accumulated_points.add(points_3d_src)
```

**1.2 Voxel Downsampling (ì¤‘ë³µ ì œê±°)**
```python
    # ì¤‘ë³µ í¬ì¸íŠ¸ ë° ë…¸ì´ì¦ˆ ì œê±°
    accumulated_points = voxel_downsample(
        accumulated_points, 
        voxel_size=0.05  # 5cm ê·¸ë¦¬ë“œ
    )
```

**1.3 Back-projection (ì¬íˆ¬ì˜)**
```python
    # ëˆ„ì ëœ 3D í¬ì¸íŠ¸ë¥¼ ê¸°ì¤€ í”„ë ˆì„ì— íˆ¬ì˜
    # 1. World â†’ Reference Camera ë³€í™˜
    points_3d_ref = T_world_to_ref @ accumulated_points
    
    # 2. 3D â†’ 2D Projection
    pixels_2d, colors = project(
        points_3d_ref, 
        K_ref, 
        image_size=(H, W)
    )
    
    # 3. Warped ì´ë¯¸ì§€ ìƒì„±
    warped_image = np.zeros((H, W, 3), dtype=np.uint8)
    for (x, y), color in zip(pixels_2d, colors):
        warped_image[y, x] = color
    
    # 4. êµ¬ë© ì±„ìš°ê¸° (ì›ë³¸ ì •ì  ì˜ì—­ + Warped ë°°ê²½)
    ref_mask = load_mask(ref_frame_idx)
    final_image = ref_image.copy()
    
    # ë™ì  ê°ì²´ ì˜ì—­ë§Œ Warpedë¡œ êµì²´
    hole_mask = (ref_mask == 0)
    final_image[hole_mask] = warped_image[hole_mask]
    
    # 5. ì €ì¥
    save_image(f"step1_warped/frame_{ref_frame_idx}.png", final_image)
```

#### Input/Output

**Input:**
- `images/*.jpg`: ì›ë³¸ ì´ë¯¸ì§€
- `masks/*.png`: ë™ì  ê°ì²´ ë§ˆìŠ¤í¬
- `poses/*.json`: ì¹´ë©”ë¼ í¬ì¦ˆ
- `depth_maps/*.png` (ì„ íƒ): LiDAR ê¹Šì´

**Output:**
```
step1_warped/
â””â”€â”€ *.png  # ì‹œê³„ì—´ ëˆ„ì ìœ¼ë¡œ êµ¬ë©ì´ ì±„ì›Œì§„ ì´ë¯¸ì§€
```

**íŠ¹ì§•:**
- ì‹¤ì œ ë°°ê²½ ë°ì´í„° ì‚¬ìš© (ìƒì„± ì•„ë‹˜)
- Photo-realistic (ì›ë³¸ í”½ì…€ í™œìš©)
- í•˜ì§€ë§Œ ì™„ì „íˆ ëª» ì±„ìš¸ ìˆ˜ ìˆìŒ (Occlusion)

---

### **Step 2: Geometric Guide Generation (ê¸°í•˜í•™ì  ê°€ì´ë“œ ìƒì„±)**
(`step2_geometric_guide.py`)

**í•™ìŠµ ëª¨ë¸:** âŒ ì—†ìŒ (RANSAC í‰ë©´ ì¶”ì • ê¸°ë°˜)

#### ëª©ì 
**Step 1ì—ì„œ ëª» ì±„ìš´ êµ¬ë©ì— ëŒ€í•œ ê¸°í•˜í•™ì  íŒíŠ¸ ì œê³µ (Step 3 ControlNet ì…ë ¥)**

#### í•µì‹¬ ì•„ì´ë””ì–´
```
ë„ë¡œ í™˜ê²½ = ì£¼ë¡œ í‰ë©´
â†’ RANSAC í‰ë©´ ì¶”ì •ìœ¼ë¡œ êµ¬ë© ì˜ì—­ì˜ depth ì˜ˆì¸¡
â†’ ControlNetì´ ì´ë¥¼ ê°€ì´ë“œë¡œ ì‚¬ìš©
```

#### Process

**2.1 êµ¬ë© ì˜ì—­ íƒì§€**
```python
for frame_idx in range(len(frames)):
    # 1. Step 1 ê²°ê³¼ ë¡œë“œ
    warped_image = load_image(f"step1_warped/frame_{frame_idx}.png")
    original_mask = load_mask(f"masks/frame_{frame_idx}.png")
    
    # 2. ì—¬ì „íˆ ê²€ì€ìƒ‰ì¸ ê³³ = Step 1 ì‹¤íŒ¨ ì˜ì—­
    still_missing = (np.sum(warped_image, axis=2) == 0)
    
    # 3. ìµœì¢… êµ¬ë© ë§ˆìŠ¤í¬ = ì›ë˜ ë™ì  ì˜ì—­ AND Step 1 ì‹¤íŒ¨
    hole_mask = (original_mask == 0) & still_missing
```

**2.2 LiDAR Depth í™œìš© (if available)**
```python
    if use_lidar:
        # LiDAR depth map ë¡œë“œ
        lidar_depth = load_depth(f"depth_maps/frame_{frame_idx}.png")
        
        # ìœ íš¨í•œ depth ì˜ì—­ì—ì„œ í‰ë©´ ì¶”ì •
        valid_depth_mask = (lidar_depth > 0) & (~hole_mask)
        valid_points = lidar_depth[valid_depth_mask]
```

**2.3 í‰ë©´ ì¶”ì • (RANSAC)**
```python
    # ë°”ë‹¥ ì˜ì—­ ì„ íƒ (ì´ë¯¸ì§€ í•˜ë‹¨ 60%)
    bottom_region = image[int(H * 0.4):, :]
    bottom_depth = lidar_depth[int(H * 0.4):, :]
    
    # 2D ì¢Œí‘œ + Depth â†’ 3D í¬ì¸íŠ¸
    points_3d = []
    for y, x in zip(*np.where(bottom_depth > 0)):
        Z = bottom_depth[y, x]
        X = (x - cx) * Z / fx
        Y = (y - cy) * Z / fy
        points_3d.append([X, Y, Z])
    
    points_3d = np.array(points_3d)
    
    # RANSAC í‰ë©´ í”¼íŒ…
    # í‰ë©´ ë°©ì •ì‹: aX + bY + cZ + d = 0
    plane_model, inliers = fit_plane_ransac(
        points_3d,
        distance_threshold=0.05,  # 5cm
        max_iterations=1000
    )
    
    a, b, c, d = plane_model
```

**2.4 êµ¬ë© ì˜ì—­ Depth ì˜ˆì¸¡**
```python
    # êµ¬ë© ì˜ì—­ì˜ ê° í”½ì…€ì— ëŒ€í•´ í‰ë©´ ë°©ì •ì‹ìœ¼ë¡œ depth ê³„ì‚°
    depth_guide = lidar_depth.copy()
    
    for y, x in zip(*np.where(hole_mask)):
        # í‰ë©´ êµì  ê³„ì‚°
        # Ray: (x-cx)/fx * t, (y-cy)/fy * t, t
        # Plane: a*X + b*Y + c*Z + d = 0
        
        X_dir = (x - cx) / fx
        Y_dir = (y - cy) / fy
        Z_dir = 1.0
        
        # t = -d / (a*X_dir + b*Y_dir + c*Z_dir)
        t = -d / (a * X_dir + b * Y_dir + c * Z_dir)
        
        if t > 0:  # ì¹´ë©”ë¼ ì•
            depth_guide[y, x] = t
```

**2.5 ë³´ê°„ ë° ì •ì œ**
```python
    # ë¶€ë“œëŸ¬ìš´ ì „í™˜ì„ ìœ„í•œ ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬
    kernel_size = 15
    depth_guide = cv2.GaussianBlur(depth_guide, (kernel_size, kernel_size), 0)
    
    # uint16 ë³€í™˜ (mm ë‹¨ìœ„)
    depth_guide_mm = (depth_guide * 1000).astype(np.uint16)
    
    # ì €ì¥
    save_depth(f"step2_depth_guide/frame_{frame_idx}.png", depth_guide_mm)
    save_mask(f"step2_hole_masks/frame_{frame_idx}.png", hole_mask.astype(np.uint8) * 255)
```

#### Input/Output

**Input:**
- `step1_warped/*.png`: Step 1 ê²°ê³¼
- `masks/*.png`: ì›ë³¸ ë™ì  ê°ì²´ ë§ˆìŠ¤í¬
- `depth_maps/*.png` (ì„ íƒ): LiDAR ê¹Šì´
- `poses/*.json`: ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„°

**Output:**
```
step2_depth_guide/
â””â”€â”€ *.png  # êµ¬ë© ì˜ì—­ì˜ depth guide (uint16)

step2_hole_masks/
â””â”€â”€ *.png  # Step 1ì—ì„œ ëª» ì±„ìš´ êµ¬ë© ë§ˆìŠ¤í¬ (uint8)
```

**íŠ¹ì§•:**
- ê¸°í•˜í•™ì ìœ¼ë¡œ ê·¸ëŸ´ë“¯í•œ(plausible) depth
- ControlNetì˜ structure guidanceë¡œ í™œìš©
- í‰ë©´ ê°€ì • (ë„ë¡œì— ì í•©)

---

### **Step 3: Final Inpainting (ìµœì¢… AI ìƒì„±)**
(`step3_final_inpainting.py`)

**í•™ìŠµ ëª¨ë¸:** âœ… 3ê°€ì§€ (ì‚¬ì „í•™ìŠµ ëª¨ë¸ + ì„ íƒì  Fine-tuning)

| ëª¨ë¸ | ìš©ë„ | í•™ìŠµ í•„ìš” | í¬ê¸° |
|-----|------|----------|------|
| **Stable Diffusion 1.5** | Base ìƒì„± ëª¨ë¸ | âŒ ì‚¬ì „í•™ìŠµ ì‚¬ìš© | ~4GB |
| **ControlNet (Depth)** | ê¸°í•˜í•™ì  ì œì•½ | âŒ ì‚¬ì „í•™ìŠµ ì‚¬ìš© | ~1.5GB |
| **LoRA** | Waymo ë„ë©”ì¸ íŠ¹í™” | âœ… ì„ íƒì  í•™ìŠµ | ~10MB |

#### ëª©ì 
**Stable Diffusion + ControlNet + LoRAë¡œ ê³ í’ˆì§ˆ ìµœì¢… ì¸í˜ì¸íŒ…**

#### í•µì‹¬ ì•„ì´ë””ì–´
```
Step 1 (ì‹¤ì œ ë°°ê²½) + Step 2 (ê¸°í•˜í•™ì  ê°€ì´ë“œ) + AI ìƒì„±
â†’ Photo-realistic & Geometrically consistent
```

#### Process

**3.1 ëª¨ë¸ ì´ˆê¸°í™”**
```python
from diffusers import (
    StableDiffusionControlNetInpaintPipeline,
    ControlNetModel,
    UniPCMultistepScheduler
)

# 1. ControlNet ë¡œë“œ (Depth ê°€ì´ë“œ)
controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/control_v11f1p_sd15_depth",
    torch_dtype=torch.float16
)

# 2. Stable Diffusion 1.5 ë¡œë“œ
pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    controlnet=controlnet,
    torch_dtype=torch.float16,
    safety_checker=None
)

# 3. ìŠ¤ì¼€ì¤„ëŸ¬ ìµœì í™” (ì†ë„ 2ë°°)
pipe.scheduler = UniPCMultistepScheduler.from_config(
    pipe.scheduler.config
)

# 4. LoRA ë¡œë“œ (Waymo ì „ìš© í•™ìŠµ ê°€ì¤‘ì¹˜)
if lora_path:
    pipe.load_lora_weights(lora_path)
    trigger_word = "WaymoStyle road"
else:
    trigger_word = "high quality realistic asphalt road"
```

**3.2 í”„ë ˆì„ë³„ ì¸í˜ì¸íŒ…**
```python
for frame_idx in range(len(frames)):
    # 1. Load Inputs
    warped_img = load_image(f"step1_warped/frame_{frame_idx}.png")
    depth_guide = load_depth(f"step2_depth_guide/frame_{frame_idx}.png")
    hole_mask = load_mask(f"step2_hole_masks/frame_{frame_idx}.png")
    original_img = load_image(f"images/frame_{frame_idx}.jpg")
    original_mask = load_mask(f"masks/frame_{frame_idx}.png")
    
    # 2. Base Image ìƒì„±
    # ì›ë³¸ + Step 1 ê²°ê³¼ í•©ì„±
    base_image = original_img.copy()
    valid_warped = (np.sum(warped_img, axis=2) > 0)  # Warpedê°€ ìœ íš¨í•œ ê³³
    mask_bool = (original_mask == 0)  # ë™ì  ê°ì²´ ì˜ì—­
    base_image[mask_bool & valid_warped] = warped_img[mask_bool & valid_warped]
    
    # 3. ìµœì¢… ë§ˆìŠ¤í¬: Step 1ë¡œë„ ëª» ì±„ìš´ ì§„ì§œ êµ¬ë©
    final_mask = cv2.bitwise_and(original_mask == 0, hole_mask > 0)
```

**3.3 Preprocessing (Numpy â†’ PIL)**
```python
    # PIL ë³€í™˜
    img_pil = Image.fromarray(cv2.cvtColor(base_image, cv2.COLOR_BGR2RGB))
    mask_pil = Image.fromarray(final_mask.astype(np.uint8) * 255)
    
    # Depth Normalization (ControlNet ì…ë ¥ í˜•ì‹)
    depth_norm = (depth_guide / 65535.0 * 255).astype(np.uint8)
    depth_pil = Image.fromarray(np.stack([depth_norm]*3, axis=-1))  # 3ì±„ë„
```

**3.4 Prompt Engineering**
```python
    # ììœ¨ì£¼í–‰ ë„ë¡œì— íŠ¹í™”ëœ í”„ë¡¬í”„íŠ¸
    positive_prompt = (
        f"{trigger_word}, sharp focus, photorealistic, 8k uhd, "
        f"detailed pavement texture, driving scene, clear lane markings"
    )
    
    negative_prompt = (
        "blur, low quality, artifacts, watermark, text, "
        "cars, pedestrians, objects, obstacles, distortions"
    )
```

**3.5 Inference (Diffusion)**
```python
    with torch.inference_mode():
        result_pil = pipe(
            prompt=positive_prompt,
            negative_prompt=negative_prompt,
            image=img_pil,
            mask_image=mask_pil,
            control_image=depth_pil,  # ControlNet depth guide
            num_inference_steps=20,    # UniPC: 20 stepìœ¼ë¡œ ì¶©ë¶„
            guidance_scale=7.5,
            controlnet_conditioning_scale=0.8,  # Depth ì˜í–¥ë„
            strength=1.0  # ë§ˆìŠ¤í¬ ì˜ì—­ ì™„ì „ ì¬ìƒì„±
        ).images[0]
```

**3.6 Post-processing (PIL â†’ Numpy)**
```python
    result_np = cv2.cvtColor(np.array(result_pil), cv2.COLOR_RGB2BGR)
    
    # ì›ë³¸ ë³´ì¡´ (ë§ˆìŠ¤í¬ ë°”ê¹¥ì€ ì›ë³¸ ìœ ì§€)
    final_image = base_image.copy()
    final_image[final_mask > 0] = result_np[final_mask > 0]
    
    # ì €ì¥
    save_image(f"step3_final_inpainted/frame_{frame_idx}.jpg", final_image)
```

**3.7 Finalization**
```python
# step3_final_inpainted/ â†’ final_inpainted/ ë³µì‚¬
import shutil
for file in Path("step3_final_inpainted").glob("*.jpg"):
    shutil.copy(file, f"final_inpainted/{file.name}")
```

#### Input/Output

**Input:**
- `step1_warped/*.png`: Step 1 ì‹œê³„ì—´ ëˆ„ì  ê²°ê³¼
- `step2_depth_guide/*.png`: Step 2 ê¹Šì´ ê°€ì´ë“œ
- `step2_hole_masks/*.png`: ìµœì¢… êµ¬ë© ë§ˆìŠ¤í¬
- `images/*.jpg`: ì›ë³¸ ì´ë¯¸ì§€ (ì°¸ì¡°ìš©)
- `masks/*.png`: ì›ë³¸ ë™ì  ê°ì²´ ë§ˆìŠ¤í¬

**Output:**
```
step3_final_inpainted/
â””â”€â”€ *.jpg  # AI ìƒì„± ìµœì¢… ê²°ê³¼

final_inpainted/  # Approach 1ê³¼ ë™ì¼ ê²½ë¡œ
â””â”€â”€ *.jpg  # ìµœì¢… ì¶œë ¥ (ë³µì‚¬ë³¸)
```

**íŠ¹ì§•:**
- Stable Diffusionì˜ ìƒì„± ëŠ¥ë ¥ í™œìš©
- ControlNetìœ¼ë¡œ ê¸°í•˜í•™ì  ì œì•½
- LoRAë¡œ Waymo ë„ë©”ì¸ íŠ¹í™”

---

### ğŸ“¤ Final Output

```
final_inpainted/
â”œâ”€â”€ frame_000.jpg
â”œâ”€â”€ frame_001.jpg
â””â”€â”€ ...
```

**ìƒì„± ê³¼ì •:**
1. **Step 1**: ì‹¤ì œ ë°°ê²½ í”½ì…€ ìµœëŒ€í•œ í™œìš© (Photo-realistic)
2. **Step 2**: ë‚¨ì€ êµ¬ë©ì— ê¸°í•˜í•™ì  íŒíŠ¸
3. **Step 3**: AIê°€ íŒíŠ¸ ê¸°ë°˜ìœ¼ë¡œ ê³ í’ˆì§ˆ ìƒì„±

---

### ğŸ¯ ì¥ë‹¨ì 

#### âœ… ì¥ì 
1. **ì†ë„**: COLMAPë³´ë‹¤ 5-10ë°° ë¹ ë¦„ (~10-30ë¶„)
2. **í’ˆì§ˆ**: AI ìƒì„±ìœ¼ë¡œ í…ìŠ¤ì²˜ í’ë¶€
3. **ì™„ì „ì„±**: í•­ìƒ ëª¨ë“  êµ¬ë© ì±„ì›€ (ìƒì„±ì´ë¯€ë¡œ)
4. **ìœ ì—°ì„±**: LoRAë¡œ ë„ë©”ì¸ íŠ¹í™” ê°€ëŠ¥
5. **ì‹¤ì œ ë°ì´í„° ìš°ì„ **: Step 1ì—ì„œ ì›ë³¸ í™œìš©

#### âš ï¸ ë‹¨ì 
1. **í•™ìŠµ ì˜ì¡´**: Stable Diffusion ëª¨ë¸ í•„ìš” (~4GB)
2. **GPU í•„ìˆ˜**: Inferenceì— GPU í•„ìš”
3. **Multi-view Consistency**: ë³´ì¥ ì•ˆ ë¨ (í”„ë ˆì„ë³„ ë…ë¦½ ìƒì„±)
4. **Hallucination**: AIê°€ ì˜ëª»ëœ íŒ¨í„´ ìƒì„± ê°€ëŠ¥

---

### ğŸ“Š ì„±ëŠ¥ ì§€í‘œ

| í•­ëª© | ê°’ | ì„¤ëª… |
|-----|---|------|
| **ì²˜ë¦¬ ì†ë„** | ~10-30ë¶„ | 100 í”„ë ˆì„ ê¸°ì¤€ (GPU) |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©** | ~6GB VRAM | Stable Diffusion 1.5 |
| **Step 1 ë³µì›ë¥ ** | ~70-85% | ì‹œê³„ì—´ë¡œ ì±„ìš¸ ìˆ˜ ìˆëŠ” ë¹„ìœ¨ |
| **Step 3 í’ˆì§ˆ** | LPIPS ~0.1 | ì›ë³¸ ëŒ€ë¹„ perceptual distance |
| **ì™„ì „ì„±** | 100% | ëª¨ë“  êµ¬ë© ì±„ì›€ ë³´ì¥ |

---

## ğŸ“Š ë‘ Approach ë¹„êµ

| í•­ëª© | Approach 1 (COLMAP) | Approach 2 (Sequential) |
|-----|---------------------|------------------------|
| **í•µì‹¬ ì „ëµ** | 3D ì¬êµ¬ì„± ê¸°ë°˜ | ì‹œê³„ì—´ + AI ìƒì„± |
| **ì²˜ë¦¬ ì†ë„** | ëŠë¦¼ (~1-5ì‹œê°„) | ë¹ ë¦„ (~10-30ë¶„) |
| **Multi-view Consistency** | âœ… ìë™ ë³´ì¥ | âš ï¸ ë³´ì¥ ì•ˆ ë¨ |
| **í…ìŠ¤ì²˜ í’ˆì§ˆ** | ë³´í†µ (ì›ë³¸ ì¬ì‚¬ìš©) | ë†’ìŒ (AI ìƒì„±) |
| **ì™„ì „ì„±** | âš ï¸ ì¬êµ¬ì„± ì‹¤íŒ¨ ì‹œ êµ¬ë© | âœ… 100% ì±„ì›€ |
| **GPU ìš”êµ¬** | ì„ íƒì  | í•„ìˆ˜ (Step 3) |
| **ì™¸ë¶€ ì˜ì¡´ì„±** | COLMAP í•„ìˆ˜ | PyTorch, Diffusers |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©** | ë†’ìŒ (8-16GB) | ì¤‘ê°„ (6GB VRAM) |
| **ë„ë©”ì¸ íŠ¹í™”** | ë¶ˆê°€ëŠ¥ | âœ… LoRAë¡œ ê°€ëŠ¥ |
| **ê¸°í•˜í•™ì  ì •í™•ë„** | ë§¤ìš° ë†’ìŒ | ë†’ìŒ (ControlNet) |
| **ì ìš© í™˜ê²½** | êµ¬ì¡°ì  ì¬êµ¬ì„± ì¤‘ìš” | ë¹ ë¥¸ ì²˜ë¦¬ í•„ìš” |

---

## ğŸ”„ í†µí•© ë°ì´í„° í”Œë¡œìš°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Preprocessing Stage Output              â”‚
â”‚  - images/                                  â”‚
â”‚  - masks/ (ë™ì  ê°ì²´ ë§ˆìŠ¤í¬)                 â”‚
â”‚  - poses/                                   â”‚
â”‚  - depth_maps/ (LiDAR)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
        â”‚             â”‚
        â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Approach 1   â”‚  â”‚ Approach 2               â”‚
â”‚ COLMAP-based â”‚  â”‚ Sequential Pipeline      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                 â”‚
       â”‚                 â”œâ”€â–º Step 1: Temporal Accumulation
       â”‚                 â”‚    â†’ step1_warped/
       â”‚                 â”‚
       â”œâ”€â–º Feature       â”œâ”€â–º Step 2: Geometric Guide
       â”‚   Extraction    â”‚    â†’ step2_depth_guide/
       â”‚                 â”‚    â†’ step2_hole_masks/
       â”œâ”€â–º SfM           â”‚
       â”‚   (sparse/)     â”œâ”€â–º Step 3: AI Inpainting
       â”‚                 â”‚    â†’ step3_final_inpainted/
       â”œâ”€â–º MVS           â”‚
       â”‚   (dense/)      â”‚
       â”‚                 â”‚
       â”œâ”€â–º Hole Filling  â”‚
       â”‚                 â”‚
       â”œâ”€â–º Post-process  â”œâ”€â–º Copy to final output
       â”‚                 â”‚
       â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         final_inpainted/                     â”‚
â”‚  ë™ì  ê°ì²´ê°€ ì œê±°ëœ ìµœì¢… ë°°ê²½ ì´ë¯¸ì§€           â”‚
â”‚  (ë‘ Approach ëª¨ë‘ ë™ì¼ ê²½ë¡œì— ì¶œë ¥)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ ê° Approachì˜ ì í•©í•œ ì‚¬ìš© ì‚¬ë¡€

### Approach 1 (COLMAP) ì¶”ì²œ ì‹œë‚˜ë¦¬ì˜¤

âœ… **ë‹¤ìŒ ê²½ìš°ì— ì‚¬ìš©:**
1. **3D ì¬êµ¬ì„±ì´ ìµœì¢… ëª©í‘œ**ì¸ ê²½ìš° (NeRF, 3DGS í•™ìŠµìš©)
2. **Multi-view consistencyê°€ í•„ìˆ˜**ì¸ ê²½ìš°
3. **ì‹œê°„ ì œì•½ì´ ì—†ëŠ”** ê²½ìš° (ë°°ì¹˜ ì²˜ë¦¬)
4. **ê¸°í•˜í•™ì  ì •í™•ë„ê°€ ë§¤ìš° ì¤‘ìš”**í•œ ê²½ìš°
5. ì •ì  ì˜ì—­ì´ **ì¶©ë¶„íˆ ë§ì•„** SfMì´ ì„±ê³µí•  ìˆ˜ ìˆëŠ” ê²½ìš°

**ì˜ˆì‹œ:**
- ììœ¨ì£¼í–‰ ì‹œë®¬ë ˆì´í„°ìš© 3D í™˜ê²½ ìƒì„±
- Novel View Synthesis ì—°êµ¬
- ë””ì§€í„¸ íŠ¸ìœˆ êµ¬ì¶•

---

### Approach 2 (Sequential) ì¶”ì²œ ì‹œë‚˜ë¦¬ì˜¤

âœ… **ë‹¤ìŒ ê²½ìš°ì— ì‚¬ìš©:**
1. **ë¹ ë¥¸ ì²˜ë¦¬ê°€ í•„ìš”**í•œ ê²½ìš° (ì‹¤ì‹œê°„ì— ê°€ê¹Œìš´)
2. **2D ì´ë¯¸ì§€ë§Œ í•„ìš”**í•œ ê²½ìš° (3D ë¶ˆí•„ìš”)
3. **GPU ì‚¬ìš© ê°€ëŠ¥**í•œ ê²½ìš°
4. **í…ìŠ¤ì²˜ í’ˆì§ˆì´ ì¤‘ìš”**í•œ ê²½ìš°
5. **ë„ë©”ì¸ íŠ¹í™”** (LoRA í™œìš©) ì›í•˜ëŠ” ê²½ìš°

**ì˜ˆì‹œ:**
- ììœ¨ì£¼í–‰ ë°ì´í„° ì¦ê°• (Data Augmentation)
- ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ì •ì œ
- Inpainting ë²¤ì¹˜ë§ˆí¬ ìƒì„±
- NeRF í•™ìŠµìš© ì „ì²˜ë¦¬ (ë°°ê²½ë§Œ ë¶„ë¦¬)

---

## ğŸ”§ ëª¨ë¸ ì„¤ì¹˜ ê°€ì´ë“œ

### Approach 1: COLMAP ì„¤ì¹˜
```bash
# Ubuntu/Debian
sudo apt-get install colmap

# macOS
brew install colmap

# Windows
# Download from: https://github.com/colmap/colmap/releases

# Verify installation
colmap --help
```

---

### Approach 2: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

#### ë°©ë²• 1: ìë™ ë‹¤ìš´ë¡œë“œ (ê¶Œì¥)
```python
# Python ìŠ¤í¬ë¦½íŠ¸ë¡œ í•œ ë²ˆì— ë‹¤ìš´ë¡œë“œ
from diffusers import StableDiffusionControlNetInpaintPipeline, ControlNetModel
import torch

print("Downloading Stable Diffusion 1.5...")
pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16
)

print("Downloading ControlNet Depth...")
controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/control_v11f1p_sd15_depth",
    torch_dtype=torch.float16
)

print("Models downloaded successfully!")
print(f"Location: ~/.cache/huggingface/")
```

#### ë°©ë²• 2: CLIë¡œ ë‹¤ìš´ë¡œë“œ
```bash
# Hugging Face CLI ì„¤ì¹˜
pip install huggingface-hub

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
huggingface-cli download runwayml/stable-diffusion-v1-5
huggingface-cli download lllyasviel/control_v11f1p_sd15_depth
```

#### ë°©ë²• 3: ì²« ì‹¤í–‰ ì‹œ ìë™ ë‹¤ìš´ë¡œë“œ
```bash
# ì²˜ìŒ ì‹¤í–‰í•˜ë©´ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë¨
python Inpainting/approach2_sequential.py /path/to/data
# -> ëª¨ë¸ì´ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ ì‹œì‘
```

---

### LoRA í•™ìŠµ (ì„ íƒì )

#### í•™ìŠµ ë°ì´í„° ì¤€ë¹„
```bash
# Waymo ë„ë¡œ sceneì—ì„œ í•™ìŠµ ë°ì´í„° ìƒì„±
python Inpainting/training_dataset_builder.py \
    --data_root /path/to/waymo_data \
    --output_dir ./lora_training_data \
    --num_samples 1000
```

#### LoRA í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (ì˜ˆì‹œ)
```bash
# diffusers í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©
# https://github.com/huggingface/diffusers/tree/main/examples/dreambooth

accelerate launch train_dreambooth_lora.py \
  --pretrained_model_name_or_path="runwayml/stable-diffusion-v1-5" \
  --instance_data_dir="./lora_training_data" \
  --output_dir="./trained_lora" \
  --instance_prompt="WaymoStyle road" \
  --resolution=512 \
  --train_batch_size=4 \
  --gradient_accumulation_steps=1 \
  --learning_rate=1e-4 \
  --lr_scheduler="constant" \
  --lr_warmup_steps=0 \
  --max_train_steps=1000 \
  --rank=16
```

#### í•™ìŠµëœ LoRA ì‚¬ìš©
```bash
python Inpainting/approach2_sequential.py \
    /path/to/data \
    --lora_path ./trained_lora/pytorch_lora_weights.safetensors
```

---

## ğŸš€ ì‹¤í–‰ ì˜ˆì‹œ

### Approach 1: COLMAP-based
```bash
# 1. COLMAP ì„¤ì¹˜ í™•ì¸
colmap --help

# 2. ì‹¤í–‰
python Inpainting/approach1_colmap.py \
    /path/to/waymo_nre_data \
    --colmap_path colmap

# 3. ì¶œë ¥ í™•ì¸
ls /path/to/waymo_nre_data/final_inpainted/
```

---

### Approach 2: Sequential
```bash
# 1. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ)
python -c "from diffusers import StableDiffusionControlNetInpaintPipeline; \
           StableDiffusionControlNetInpaintPipeline.from_pretrained('runwayml/stable-diffusion-v1-5')"

# 2. ì‹¤í–‰ (ê¸°ë³¸ ì„¤ì •)
python Inpainting/approach2_sequential.py \
    /path/to/waymo_nre_data

# 3. ì‹¤í–‰ (ê³ ê¸‰ ì„¤ì •)
python Inpainting/approach2_sequential.py \
    /path/to/waymo_nre_data \
    --voxel_size 0.03 \
    --sample_interval 3 \
    --ground_ratio 0.65 \
    --lora_path ./trained_lora/waymo_road.safetensors

# 4. ì¶œë ¥ í™•ì¸
ls /path/to/waymo_nre_data/final_inpainted/
```

---

### ë‹¨ê³„ë³„ ì‹¤í–‰ (Approach 2)
```bash
# Step 1ë§Œ
python Inpainting/step1_temporal_accumulation.py \
    --data_root /path/to/data

# Step 2ë§Œ (Step 1 ì´í›„)
python Inpainting/step2_geometric_guide.py \
    --data_root /path/to/data

# Step 3ë§Œ (Step 1, 2 ì´í›„)
python Inpainting/step3_final_inpainting.py \
    --data_root /path/to/data \
    --lora_path ./lora_weights.safetensors
```

---

## ğŸ¤– í•™ìŠµ ëª¨ë¸ ì¢…í•© ì •ë¦¬

### Approach 1: COLMAP-based
**í•™ìŠµ ëª¨ë¸ ì‚¬ìš©:** âŒ ì—†ìŒ

| Step | ê¸°ë²• | í•™ìŠµ í•„ìš” | ì„¤ëª… |
|------|------|----------|------|
| Feature Extraction | SIFT | âŒ | Hand-crafted feature |
| Feature Matching | Brute-force / KNN | âŒ | ì „í†µì  matching |
| SfM | Bundle Adjustment | âŒ | Optimization ê¸°ë°˜ |
| MVS | Patch Match Stereo | âŒ | Photo-consistency |
| Hole Filling | Depth-based Inpaint | âŒ | OpenCV ì•Œê³ ë¦¬ì¦˜ |

**ì¥ì :**
- ëª¨ë¸ ë‹¤ìš´ë¡œë“œ/í•™ìŠµ ë¶ˆí•„ìš”
- COLMAPë§Œ ì„¤ì¹˜í•˜ë©´ ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥
- ì¬í˜„ì„± 100% ë³´ì¥

---

### Approach 2: Sequential Pipeline
**í•™ìŠµ ëª¨ë¸ ì‚¬ìš©:** âœ… Step 3ì—ì„œë§Œ

#### Step 1: Temporal Accumulation
**í•™ìŠµ ëª¨ë¸:** âŒ ì—†ìŒ

| ê¸°ë²• | ì„¤ëª… |
|-----|------|
| 3D Point Cloud Accumulation | ê¸°í•˜í•™ì  íˆ¬ì˜ |
| Voxel Downsampling | Grid ê¸°ë°˜ ìƒ˜í”Œë§ |
| Back-projection | ì¹´ë©”ë¼ ëª¨ë¸ ê¸°ë°˜ íˆ¬ì˜ |

---

#### Step 2: Geometric Guide
**í•™ìŠµ ëª¨ë¸:** âŒ ì—†ìŒ

| ê¸°ë²• | ì„¤ëª… |
|-----|------|
| RANSAC Plane Fitting | Robust í‰ë©´ ì¶”ì • |
| Depth Interpolation | ê¸°í•˜í•™ì  ë³´ê°„ |

---

#### Step 3: Final Inpainting
**í•™ìŠµ ëª¨ë¸:** âœ… 3ê°€ì§€

##### 1ï¸âƒ£ Stable Diffusion 1.5
```python
Model: "runwayml/stable-diffusion-v1-5"
Type: Text-to-Image Diffusion Model
```

**ì‚¬ì–‘:**
- **í¬ê¸°:** ~4GB (fp16)
- **ì•„í‚¤í…ì²˜:** U-Net + VAE + Text Encoder (CLIP)
- **í•™ìŠµ ë°ì´í„°:** LAION-5B
- **ì…ë ¥:** Text prompt + (ì„ íƒ) Image
- **ì¶œë ¥:** 512Ã—512 ì´ë¯¸ì§€

**ì‚¬ìš© ë°©ì‹:**
- âŒ ì¬í•™ìŠµ ë¶ˆí•„ìš”
- âœ… ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë§Œ
- ğŸ”„ Inpainting ëª¨ë“œ ì§€ì›

**ë‹¤ìš´ë¡œë“œ:**
```python
from diffusers import StableDiffusionControlNetInpaintPipeline

pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16
)
# ìë™ ë‹¤ìš´ë¡œë“œ: ~/.cache/huggingface/
```

---

##### 2ï¸âƒ£ ControlNet (Depth)
```python
Model: "lllyasviel/control_v11f1p_sd15_depth"
Type: Depth-conditioned Control Network
```

**ì‚¬ì–‘:**
- **í¬ê¸°:** ~1.5GB (fp16)
- **ì•„í‚¤í…ì²˜:** U-Net encoder (SDì™€ ê³µìœ  êµ¬ì¡°)
- **í•™ìŠµ ë°ì´í„°:** Multi-domain depth maps
- **ì…ë ¥:** Depth image (3ì±„ë„ RGB í˜•ì‹)
- **ì¶œë ¥:** SD latent space conditioning

**ì‚¬ìš© ë°©ì‹:**
- âŒ ì¬í•™ìŠµ ë¶ˆí•„ìš”
- âœ… ì‚¬ì „í•™ìŠµ ëª¨ë¸ ì‚¬ìš©
- ğŸ›ï¸ conditioning_scale ì¡°ì ˆ ê°€ëŠ¥ (0.0~1.0)

**ë‹¤ìš´ë¡œë“œ:**
```python
from diffusers import ControlNetModel

controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/control_v11f1p_sd15_depth",
    torch_dtype=torch.float16
)
```

---

##### 3ï¸âƒ£ LoRA (Low-Rank Adaptation)
```python
Model: Custom trained on Waymo dataset
Type: Fine-tuning adapter for domain adaptation
```

**ì‚¬ì–‘:**
- **í¬ê¸°:** ~10-50MB (.safetensors)
- **ì•„í‚¤í…ì²˜:** Low-rank matrices (rank=4~64)
- **í•™ìŠµ ë°ì´í„°:** Waymo road scenes (ì‚¬ìš©ì ìƒì„±)
- **ì…ë ¥:** SD latent space
- **ì¶œë ¥:** Domain-adapted features

**í•™ìŠµ í•„ìš”:** âœ… **ì„ íƒì **

**í•™ìŠµ ë°©ë²•:**
```bash
# í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„±
python training_dataset_builder.py \
    --data_root /path/to/waymo \
    --output_dir ./lora_training_data

# LoRA í•™ìŠµ (ì˜ˆì‹œ)
python train_lora.py \
    --base_model "runwayml/stable-diffusion-v1-5" \
    --train_data ./lora_training_data \
    --output_dir ./trained_lora \
    --trigger_word "WaymoStyle road" \
    --rank 16 \
    --epochs 100
```

**í•™ìŠµ ì‹œê°„:**
- ~2-4ì‹œê°„ (ë‹¨ì¼ GPU, 1000-2000 ì´ë¯¸ì§€)

**ì‚¬ìš©:**
```python
pipe.load_lora_weights("./trained_lora/final.safetensors")
prompt = "WaymoStyle road, photorealistic asphalt"
```

**LoRA í•™ìŠµ ì—†ì´ ì‚¬ìš© ê°€ëŠ¥?**
- âœ… ê°€ëŠ¥ (ê¸°ë³¸ SD 1.5 ì‚¬ìš©)
- âš ï¸ í’ˆì§ˆ: Waymo íŠ¹í™” ì—†ì´ ì¼ë°˜ì ì¸ ë„ë¡œ ìƒì„±
- ğŸ’¡ ê¶Œì¥: LoRA í•™ìŠµ ì‹œ í’ˆì§ˆ í–¥ìƒ (~10-20% LPIPS ê°œì„ )

---

### ëª¨ë¸ ì˜ì¡´ì„± ìš”ì•½

| Approach | Step | ëª¨ë¸ | í•™ìŠµ í•„ìš” | í¬ê¸° | ë‹¤ìš´ë¡œë“œ ì‹œê°„ |
|----------|------|------|----------|------|-------------|
| **1. COLMAP** | ì „ì²´ | âŒ ì—†ìŒ | - | - | - |
| **2. Sequential** | Step 1 | âŒ ì—†ìŒ | - | - | - |
| **2. Sequential** | Step 2 | âŒ ì—†ìŒ | - | - | - |
| **2. Sequential** | Step 3 | Stable Diffusion 1.5 | âŒ | 4GB | ~5-10ë¶„ |
| **2. Sequential** | Step 3 | ControlNet Depth | âŒ | 1.5GB | ~2-5ë¶„ |
| **2. Sequential** | Step 3 | LoRA (ì„ íƒ) | âœ… | 10MB | ì‚¬ìš©ì í•™ìŠµ |

**ì´ ëª¨ë¸ í¬ê¸° (Approach 2):**
- í•„ìˆ˜: ~5.5GB (SD + ControlNet)
- ì„ íƒ: +10MB (LoRA)

**ì´ˆê¸° ì„¤ì¹˜ ì‹œê°„:**
- Approach 1: ~5ë¶„ (COLMAP ì„¤ì¹˜)
- Approach 2: ~15-20ë¶„ (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ)

---

### Preprocessing Stage ëª¨ë¸ (ì°¸ê³ )

Preprocessingì˜ Dynamic Maskingì—ì„œ ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©:

#### SegFormer (Semantic Segmentation)
```python
Model: "nvidia/segformer-b0-finetuned-cityscapes-1024-1024"
```

**ì‚¬ì–‘:**
- **í¬ê¸°:** ~15MB (B0 variant)
- **í•™ìŠµ ë°ì´í„°:** Cityscapes
- **ìš©ë„:** ë™ì  ê°ì²´ ê°ì§€ (ë³´ì¡°)

**ì‚¬ìš©:**
```python
from transformers import SegformerForSemanticSegmentation

model = SegformerForSemanticSegmentation.from_pretrained(
    "nvidia/segformer-b0-finetuned-cityscapes-1024-1024"
)
```

**í•™ìŠµ í•„ìš”:** âŒ (ì‚¬ì „í•™ìŠµ ëª¨ë¸ ì‚¬ìš©)  
**ì‚¬ìš© ë¹ˆë„:** ë‚®ìŒ (3D Box íˆ¬ì˜ì´ ë” ì •í™•)

---

## ğŸ“ ì¶”ê°€ ê°œì„  ì‚¬í•­

### Approach 1
1. **Poisson Reconstruction**: Dense MVS ëŒ€ì‹  Poisson surface reconstruction
2. **Neural Rendering**: NeRF/3DGSë¡œ Novel View Synthesis
3. **Global Optimization**: Bundle Adjustment ê°•í™”

### Approach 2
1. **Multi-view Consistency Loss**: Step 3ì—ì„œ ì´ì›ƒ í”„ë ˆì„ ê³ ë ¤
2. **Diffusion Distillation**: Stable Diffusion ê²½ëŸ‰í™” (ì†ë„ â†‘)
3. **Adaptive Scheduling**: êµ¬ë© í¬ê¸°ì— ë”°ë¼ inference steps ì¡°ì ˆ
4. **Video Diffusion**: Temporal consistencyë¥¼ ìœ„í•œ ë¹„ë””ì˜¤ ëª¨ë¸ í™œìš©

---

## âœ… ìš”êµ¬ì‚¬í•­ ì¶©ì¡± í™•ì¸

| ìš”êµ¬ì‚¬í•­ | Approach 1 | Approach 2 | ìƒíƒœ |
|---------|-----------|-----------|------|
| **ë™ì  ê°ì²´ ì œê±°** | âœ… | âœ… | ì™„ë£Œ |
| **ìµœì¢… Output ë™ì¼** | âœ… `final_inpainted/` | âœ… `final_inpainted/` | ì™„ë£Œ |
| **Input ëª…ì„¸** | âœ… ë¬¸ì„œí™” | âœ… ë¬¸ì„œí™” | ì™„ë£Œ |
| **í”„ë¡œì„¸ìŠ¤ ìƒì„¸** | âœ… 6ë‹¨ê³„ | âœ… 3ë‹¨ê³„ | ì™„ë£Œ |
| **ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸** | âœ… `approach1_colmap.py` | âœ… `approach2_sequential.py` | ì™„ë£Œ |
| **ì„±ëŠ¥ ë¹„êµ** | âœ… í‘œ ì‘ì„± | âœ… í‘œ ì‘ì„± | ì™„ë£Œ |

---

---

## ğŸ“ í•™ìŠµ ëª¨ë¸ ìš”êµ¬ì‚¬í•­

### Approach 1: COLMAP-based

**âœ… í•™ìŠµ ëª¨ë¸ ë¶ˆí•„ìš”**
- ì „í†µì  ì»´í“¨í„° ë¹„ì „ ê¸°ë²• (SIFT, Bundle Adjustment, MVS)
- ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ì—†ì´ ê¸°í•˜í•™ì  ê³„ì‚°ë§Œ ì‚¬ìš©

---

### Approach 2: Sequential Multi-Stage

#### **Step 1: Temporal Accumulation**
**âœ… í•™ìŠµ ëª¨ë¸ ë¶ˆí•„ìš”**
- 3D í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ëˆ„ì  ë° íˆ¬ì˜ (ìˆœìˆ˜ ê¸°í•˜í•™)

#### **Step 2: Geometric Guide Generation**
**âœ… í•™ìŠµ ëª¨ë¸ ë¶ˆí•„ìš”**
- RANSAC í‰ë©´ ì¶”ì • (ì „í†µì  ë°©ë²•)

#### **Step 3: Final Inpainting**
**âš ï¸ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ í•„ìš” (ìë™ ë‹¤ìš´ë¡œë“œ)**

| ëª¨ë¸ | ìš©ë„ | í¬ê¸° | ë‹¤ìš´ë¡œë“œ | í•™ìŠµ í•„ìš” |
|-----|------|------|----------|----------|
| **Stable Diffusion 1.5** | Base ìƒì„± ëª¨ë¸ | ~4GB | âœ… ìë™ | âŒ ë¶ˆí•„ìš” |
| **ControlNet (Depth)** | ê¹Šì´ ê°€ì´ë“œ | ~1.5GB | âœ… ìë™ | âŒ ë¶ˆí•„ìš” |
| **LoRA (ì„ íƒ)** | Waymo ë„ë©”ì¸ íŠ¹í™” | ~10-50MB | âŒ ìˆ˜ë™ | âœ… **í•™ìŠµ í•„ìš”** |

---

### ğŸ“¦ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜

#### 1. Stable Diffusion 1.5 + ControlNet (ìë™)
```python
# ìµœì´ˆ ì‹¤í–‰ ì‹œ ìë™ ë‹¤ìš´ë¡œë“œë¨
from diffusers import StableDiffusionControlNetInpaintPipeline, ControlNetModel

# ControlNet (Depth)
controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/control_v11f1p_sd15_depth",
    torch_dtype=torch.float16
)
# â†’ ~/.cache/huggingface/hub/ ì— ìë™ ì €ì¥

# Stable Diffusion 1.5
pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    controlnet=controlnet,
    torch_dtype=torch.float16
)
# â†’ ~/.cache/huggingface/hub/ ì— ìë™ ì €ì¥
```

**ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ (ì„ íƒ):**
```bash
# Hugging Face CLI ì„¤ì¹˜
pip install huggingface-hub

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
huggingface-cli download runwayml/stable-diffusion-v1-5
huggingface-cli download lllyasviel/control_v11f1p_sd15_depth
```

---

#### 2. LoRA í•™ìŠµ (ì„ íƒì , Waymo ë„ë©”ì¸ íŠ¹í™”)

**í•„ìš”ì„±:**
- Stable Diffusionì€ ì¼ë°˜ ë„ë¡œ ì´ë¯¸ì§€ë¡œ í•™ìŠµë¨
- Waymo ë°ì´í„°ì˜ íŠ¹ì„± (ì¹´ë©”ë¼ ì™œê³¡, ì¡°ëª…, ë„ë¡œ íƒ€ì…)ì— ìµœì í™” í•„ìš”
- LoRAë¡œ ì ì€ ë°ì´í„°ë¡œ ë¹ ë¥´ê²Œ fine-tuning ê°€ëŠ¥

**í•™ìŠµ ë°ì´í„° ìƒì„±:**
```bash
# Inpainting/training_dataset_builder.py ì‚¬ìš©
python Inpainting/training_dataset_builder.py \
    /path/to/waymo_data \
    --output_dir ./lora_training_data \
    --num_samples 500
```

**Output:**
```
lora_training_data/
â”œâ”€â”€ images/           # ì›ë³¸ ë„ë¡œ ì´ë¯¸ì§€
â”œâ”€â”€ masks/            # ì¸í˜ì¸íŒ… ë§ˆìŠ¤í¬
â”œâ”€â”€ prompts.json      # í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
â””â”€â”€ metadata.json     # í•™ìŠµ ë©”íƒ€ë°ì´í„°
```

**LoRA í•™ìŠµ (diffusers ì‚¬ìš©):**
```bash
# LoRA í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì„¤ì¹˜
git clone https://github.com/huggingface/diffusers
cd diffusers/examples/text_to_image

# í•™ìŠµ ì‹¤í–‰
accelerate launch train_text_to_image_lora.py \
  --pretrained_model_name_or_path="runwayml/stable-diffusion-v1-5" \
  --train_data_dir="./lora_training_data" \
  --resolution=512 \
  --train_batch_size=4 \
  --gradient_accumulation_steps=4 \
  --max_train_steps=1000 \
  --learning_rate=1e-04 \
  --max_grad_norm=1 \
  --lr_scheduler="cosine" \
  --lr_warmup_steps=100 \
  --output_dir="./waymo_lora_output" \
  --validation_prompt="WaymoStyle road asphalt pavement" \
  --validation_epochs=50
```

**í•™ìŠµ ê²°ê³¼:**
```
waymo_lora_output/
â””â”€â”€ pytorch_lora_weights.safetensors  # í•™ìŠµëœ LoRA ê°€ì¤‘ì¹˜
```

**ì‚¬ìš©:**
```bash
python Inpainting/step3_final_inpainting.py \
    --data_root /path/to/data \
    --lora_path ./waymo_lora_output/pytorch_lora_weights.safetensors
```

---

### ğŸ”§ ëŒ€ì•ˆ: LoRA ì—†ì´ ì‚¬ìš©

**LoRA í•™ìŠµ ì—†ì´ë„ ì‚¬ìš© ê°€ëŠ¥:**
```bash
# ê¸°ë³¸ Stable Diffusionë§Œ ì‚¬ìš©
python Inpainting/step3_final_inpainting.py \
    --data_root /path/to/data
# â†’ ì¼ë°˜ì ì¸ ë„ë¡œ í…ìŠ¤ì²˜ ìƒì„± (í’ˆì§ˆì€ ì•½ê°„ ë‚®ì„ ìˆ˜ ìˆìŒ)
```

**í’ˆì§ˆ í–¥ìƒ íŒ (LoRA ì—†ì´):**
1. **Prompt Engineering:**
   ```python
   positive_prompt = "photorealistic asphalt road, high resolution, 8k, detailed texture, outdoor daylight"
   ```

2. **ControlNet Strength ì¡°ì ˆ:**
   ```python
   controlnet_conditioning_scale=0.9  # 0.8 â†’ 0.9 (ë” ê°•í•œ ê°€ì´ë“œ)
   ```

3. **Inference Steps ì¦ê°€:**
   ```python
   num_inference_steps=30  # 20 â†’ 30 (ë” ë§ì€ denoising)
   ```

---

### ğŸ“Š í•™ìŠµ ì‹œê°„ ë° ë¦¬ì†ŒìŠ¤

| í•­ëª© | LoRA í•™ìŠµ | ì„¤ëª… |
|-----|-----------|------|
| **í•™ìŠµ ë°ì´í„°** | 500-1000ì¥ | Waymoì—ì„œ ì¶”ì¶œ |
| **í•™ìŠµ ì‹œê°„** | ~2-4ì‹œê°„ | RTX 3090 ê¸°ì¤€ |
| **VRAM ìš”êµ¬** | ~12GB | Batch size 4 |
| **ìµœì¢… ëª¨ë¸ í¬ê¸°** | ~10-50MB | ì›ë³¸ ëŒ€ë¹„ 1% |
| **ì„±ëŠ¥ í–¥ìƒ** | ~10-20% | LPIPS ê¸°ì¤€ |

---

### ğŸ¯ ëª¨ë¸ í•™ìŠµ ìš°ì„ ìˆœìœ„

#### í•„ìˆ˜ (Approach 2 Step 3 ì‚¬ìš© ì‹œ)
1. âœ… **Stable Diffusion 1.5** - ìë™ ë‹¤ìš´ë¡œë“œ
2. âœ… **ControlNet (Depth)** - ìë™ ë‹¤ìš´ë¡œë“œ

#### ì„ íƒ (í’ˆì§ˆ í–¥ìƒ)
3. âš ï¸ **LoRA (Waymo íŠ¹í™”)** - ìˆ˜ë™ í•™ìŠµ í•„ìš”
   - í•™ìŠµí•˜ë©´: ë” ë‚˜ì€ í…ìŠ¤ì²˜, ë„ë©”ì¸ íŠ¹í™”
   - ì•ˆ í•˜ë©´: ê¸°ë³¸ í’ˆì§ˆë¡œë„ ì‚¬ìš© ê°€ëŠ¥

#### ì¶”ê°€ (Preprocessing)
4. âš ï¸ **SegFormer** - ìë™ ë‹¤ìš´ë¡œë“œ (Semantic Segmentation ì‚¬ìš© ì‹œ)
   ```python
   from transformers import SegformerForSemanticSegmentation
   model = SegformerForSemanticSegmentation.from_pretrained(
       "nvidia/segformer-b0-finetuned-cityscapes-1024-1024"
   )
   ```

---

### ğŸ’¾ ì „ì²´ ë””ìŠ¤í¬ ê³µê°„ ìš”êµ¬ì‚¬í•­

```
ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: ~/.cache/huggingface/hub/

í•„ìˆ˜ ëª¨ë¸:
- Stable Diffusion 1.5:        ~4.0 GB
- ControlNet (Depth):           ~1.5 GB
- Tokenizer & Scheduler:        ~0.5 GB
----------------------------------------
ì†Œê³„:                           ~6.0 GB

ì„ íƒ ëª¨ë¸:
- LoRA (í•™ìŠµ ì‹œ):               ~0.05 GB
- SegFormer (Preprocessing):    ~0.1 GB
----------------------------------------
ì´ê³„:                           ~6.2 GB
```

---

### ğŸš€ ë¹ ë¥¸ ì‹œì‘ (ëª¨ë¸ ì¤€ë¹„)

```bash
# 1. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install torch torchvision diffusers transformers accelerate safetensors

# 2. ëª¨ë¸ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ (ì„ íƒ, ìë™ìœ¼ë¡œë„ ë¨)
python -c "
from diffusers import StableDiffusionControlNetInpaintPipeline, ControlNetModel
import torch

print('Downloading ControlNet...')
controlnet = ControlNetModel.from_pretrained(
    'lllyasviel/control_v11f1p_sd15_depth',
    torch_dtype=torch.float16
)

print('Downloading Stable Diffusion 1.5...')
pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(
    'runwayml/stable-diffusion-v1-5',
    controlnet=controlnet,
    torch_dtype=torch.float16
)

print('âœ“ All models downloaded successfully!')
"

# 3. (ì„ íƒ) LoRA í•™ìŠµ
# â†’ training_dataset_builder.pyë¡œ ë°ì´í„° ìƒì„± í›„
# â†’ diffusers ì˜ˆì œë¡œ í•™ìŠµ

# 4. Inpainting ì‹¤í–‰
python Inpainting/approach2_sequential.py /path/to/data
```

---

## ğŸ¨ Style LoRA Training Pipeline

### ê°œìš”

**Waymo/KITTI ë°ì´í„°ì…‹ì˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ Stable Diffusion v1.5ì˜ ìŠ¤íƒ€ì¼(ë„ë¡œ ì§ˆê°, ìƒ‰ê°)ì„ í•™ìŠµì‹œí‚¤ëŠ” íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.**

í•™ìŠµëœ LoRA ê°€ì¤‘ì¹˜(`.safetensors`)ëŠ” Step 3 Inpaintingì—ì„œ ë„ë©”ì¸ íŠ¹í™” ìƒì„± í’ˆì§ˆì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

### íŒŒì´í”„ë¼ì¸ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Style LoRA Training Pipeline                                  â”‚
â”‚                                                                â”‚
â”‚  1. ë°ì´í„°ì…‹ ì¤€ë¹„                                              â”‚
â”‚     training_dataset_builder.py                                â”‚
â”‚     â†’ ê¹¨ë—í•œ í”„ë ˆì„ í•„í„°ë§, metadata.jsonl ìƒì„±                â”‚
â”‚                                                                â”‚
â”‚  2. LoRA í•™ìŠµ                                                  â”‚
â”‚     train_style_lora.py                                        â”‚
â”‚     â†’ SD v1.5 U-Net Attentionì— LoRA ì ìš©                     â”‚
â”‚     â†’ pytorch_lora_weights.safetensors ì¶œë ¥                    â”‚
â”‚                                                                â”‚
â”‚  3. ì¶”ë¡  & í…ŒìŠ¤íŠ¸                                              â”‚
â”‚     lora_inference.py                                          â”‚
â”‚     â†’ Text-to-Image, Inpainting, í’ˆì§ˆ í‰ê°€                    â”‚
â”‚                                                                â”‚
â”‚  4. Step 3 ì—°ë™                                                â”‚
â”‚     step3_final_inpainting.py --lora_path ...                  â”‚
â”‚     â†’ í•™ìŠµëœ LoRAë¡œ ì „ì²´ ë°ì´í„°ì…‹ ì¸í˜ì¸íŒ…                     â”‚
â”‚                                                                â”‚
â”‚  UI: lora_ui.py (Gradio ì›¹ ì¸í„°í˜ì´ìŠ¤)                         â”‚
â”‚     â†’ ìœ„ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ UIì—ì„œ ì‹¤í–‰ ê°€ëŠ¥                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### íŒŒì¼ êµ¬ì¡°

| íŒŒì¼ | ì„¤ëª… |
|------|------|
| `train_style_lora.py` | LoRA í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (StyleLoRADataset + StyleLoRATrainer) |
| `lora_inference.py` | ì¶”ë¡  & í’ˆì§ˆ í‰ê°€ (LoRAInference + LoRAQualityEvaluator) |
| `lora_ui.py` | Gradio ê¸°ë°˜ í†µí•© UI (ë°ì´í„° ì¤€ë¹„~í•™ìŠµ~ì¶”ë¡ ~í‰ê°€~Step3 ì—°ë™) |
| `training_dataset_builder.py` | í•™ìŠµ ë°ì´í„°ì…‹ ë¹Œë” (ê¸°ì¡´) |

### ë¹ ë¥¸ ì‹œì‘

#### CLI ì‚¬ìš©

```bash
# 1. LoRA í•™ìŠµ
python Inpainting/train_style_lora.py \
    --data_root /path/to/waymo_nre_format \
    --output_dir ./lora_output \
    --trigger_word "WaymoStyle road" \
    --resolution 512 \
    --max_train_steps 1000 \
    --lora_rank 16

# 2. ì¶”ë¡  í…ŒìŠ¤íŠ¸
python Inpainting/lora_inference.py generate \
    --lora_path ./lora_output/pytorch_lora_weights.safetensors \
    --prompt "WaymoStyle road, photorealistic asphalt, 8k" \
    --num_images 4

# 3. LoRA ì „/í›„ ë¹„êµ
python Inpainting/lora_inference.py compare \
    --lora_path ./lora_output/pytorch_lora_weights.safetensors \
    --output_dir ./comparison

# 4. Step 3ì— LoRA ì ìš©
python Inpainting/step3_final_inpainting.py \
    --data_root /path/to/data \
    --lora_path ./lora_output/pytorch_lora_weights.safetensors
```

#### UI ì‚¬ìš© (Gradio)

```bash
# UI ì„œë²„ ì‹¤í–‰
python Inpainting/lora_ui.py --port 7860

# ê³µìœ  ë§í¬ ìƒì„±
python Inpainting/lora_ui.py --share
```

UIëŠ” 5ê°œ íƒ­ìœ¼ë¡œ êµ¬ì„±:
1. **ë°ì´í„°ì…‹ ì¤€ë¹„**: ë””ë ‰í† ë¦¬ ìŠ¤ìº”, ì´ë¯¸ì§€ í”„ë¦¬ë·°, í•„í„°ë§
2. **LoRA í•™ìŠµ**: í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ë° í•™ìŠµ ì‹¤í–‰
3. **ì¶”ë¡  & í…ŒìŠ¤íŠ¸**: Text-to-Image, Inpainting
4. **í’ˆì§ˆ í‰ê°€**: PSNR, SSIM, ì„ ëª…ë„, LoRA ì „/í›„ ë¹„êµ
5. **Step 3 ì—°ë™**: í•™ìŠµëœ LoRAë¡œ ì „ì²´ ì¸í˜ì¸íŒ… ì‹¤í–‰

#### Python API ì‚¬ìš©

```python
# Quick-start í•¨ìˆ˜
from Inpainting.train_style_lora import train_style_lora

result = train_style_lora(
    data_root="/path/to/waymo_nre_format",
    output_dir="./lora_output",
    trigger_word="WaymoStyle road",
    max_train_steps=1000,
    lora_rank=16,
)

# ì¶”ë¡ 
from Inpainting.lora_inference import LoRAInference

infer = LoRAInference(lora_path="./lora_output")
images = infer.generate(
    "WaymoStyle road, photorealistic asphalt, sharp focus",
    num_images=4,
)

# í’ˆì§ˆ í‰ê°€
from Inpainting.lora_inference import LoRAQualityEvaluator

evaluator = LoRAQualityEvaluator()
metrics = evaluator.evaluate_generated_only(images[0])
print(metrics)  # {'sharpness': 150.2, 'brightness_mean': 128.5, ...}
```

### í•™ìŠµ ì„¤ì • ê°€ì´ë“œ

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ê¶Œì¥ ë²”ìœ„ | ì„¤ëª… |
|----------|--------|-----------|------|
| `lora_rank` | 16 | 4-64 | ë†’ì„ìˆ˜ë¡ í‘œí˜„ë ¥â†‘, ë©”ëª¨ë¦¬â†‘ |
| `learning_rate` | 1e-4 | 5e-5 ~ 2e-4 | ë„ˆë¬´ ë†’ìœ¼ë©´ ê³¼ì í•© |
| `max_train_steps` | 1000 | 500-2000 | ì´ë¯¸ì§€ ìˆ˜ì— ë¹„ë¡€ |
| `train_batch_size` | 1 | 1-4 | VRAMì— ë”°ë¼ ì¡°ì ˆ |
| `resolution` | 512 | 512-768 | SD v1.5 ê¸°ë³¸ 512 |
| `noise_offset` | 0.0 | 0.0-0.1 | ìƒ‰ ëŒ€ë¹„ í–¥ìƒ |
| `snr_gamma` | None | 5.0 | Min-SNR weighting |

### í•„ìš” íŒ¨í‚¤ì§€

```bash
# LoRA í•™ìŠµ í•„ìˆ˜
pip install torch torchvision diffusers transformers accelerate peft safetensors

# UI ì‹¤í–‰
pip install gradio

# í’ˆì§ˆ í‰ê°€
pip install opencv-python numpy pillow
```

### í•™ìŠµ ì¶œë ¥ë¬¼

```
lora_output/
â”œâ”€â”€ pytorch_lora_weights.safetensors  # ìµœì¢… LoRA ê°€ì¤‘ì¹˜ (~10-50MB)
â”œâ”€â”€ training_config.json              # í•™ìŠµ ì„¤ì • ê¸°ë¡
â”œâ”€â”€ training_log.json                 # í•™ìŠµ ë¡œê·¸ (loss history ë“±)
â”œâ”€â”€ checkpoints/                      # ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸
â”‚   â”œâ”€â”€ step_000250/
â”‚   â”œâ”€â”€ step_000500/
â”‚   â””â”€â”€ ...
â””â”€â”€ samples/                          # í•™ìŠµ ì¤‘ ìƒì„±ëœ ê²€ì¦ ìƒ˜í”Œ
    â”œâ”€â”€ step_000250.png
    â””â”€â”€ ...
```

---

**ìµœì¢… í™•ì¸ì¼**: 2026-02-09  
**ì‘ì„±ì**: Cloud Agent  
**ë²„ì „**: 2.0
