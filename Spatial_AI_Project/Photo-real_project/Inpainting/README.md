# Inpainting Module

Photo-real_projectì˜ Inpainting ëª¨ë“ˆì…ë‹ˆë‹¤. ì‹œê³„ì—´ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë™ì  ê°ì²´ê°€ ì œê±°ëœ ì˜ì—­ì„ ì •ì  ë°°ê²½ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.

## ğŸ“ êµ¬ì¡°

```
Inpainting/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ step1_temporal_accumulation.py  # ì‹œê³„ì—´ ëˆ„ì  ê¸°ë°˜ ì¸í˜ì¸íŒ…
â”œâ”€â”€ step2_geometric_guide.py        # ê¸°í•˜í•™ì  ê°€ì´ë“œ ìƒì„±
â”œâ”€â”€ step3_final_inpainting.py       # Multi-view Consistent ìµœì¢… ì¸í˜ì¸íŒ…
â”œâ”€â”€ training_dataset_builder.py     # ìƒì„±í˜• AI ëª¨ë¸ í•™ìŠµ ë°ì´í„°ì…‹ ë¹Œë”
â””â”€â”€ README.md
```

## ğŸ”„ ì›Œí¬í”Œë¡œìš°

### 1. Preprocessing (ì´ì „ ë‹¨ê³„)

ë¨¼ì € `preprocessing` íŒŒì´í”„ë¼ì¸ì„ í†µí•´ ê¸°ë³¸ ë°ì´í„°ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤:

```bash
# Waymo ë°ì´í„°ë¥¼ NRE í¬ë§·ìœ¼ë¡œ ë³€í™˜
cd preprocessing
python waymo2nre.py /path/to/waymo/raw /path/to/output --prefix seq0_

# (ì„ íƒ) SegFormerë¡œ ë§ˆìŠ¤í¬ ìƒì„±
python run_preprocessing.py /path/to/output --use_segformer
```

**Preprocessing ì¶œë ¥:**
- `images/`: ì›ë³¸ ì´ë¯¸ì§€ (JPEG/PNG)
- `masks/`: ë™ì  ê°ì²´ ë§ˆìŠ¤í¬ (0=ë™ì , 255=ì •ì )
- `poses/`: ì¹´ë©”ë¼ pose JSON íŒŒì¼
- `depths/`: (ì„ íƒ) LiDAR depth ë§µ

### 2. Step 1: ì‹œê³„ì—´ ëˆ„ì  (Temporal Accumulation)

ì—¬ëŸ¬ í”„ë ˆì„ì˜ ì •ì  ì˜ì—­ì„ 3Dë¡œ ëˆ„ì í•œ í›„ ë‹¤ì‹œ íˆ¬ì˜í•˜ì—¬ êµ¬ë©ì„ ì±„ì›ë‹ˆë‹¤.

```bash
cd ../Inpainting
python step1_temporal_accumulation.py /path/to/preprocessing/output
```

**ì˜µì…˜:**
- `--voxel_size`: Voxel downsampling í¬ê¸° (ë¯¸í„°, ê¸°ë³¸ê°’: 0.05)
- `--sample_interval`: Forward pass ìƒ˜í”Œë§ ê°„ê²© (ê¸°ë³¸ê°’: 5)

**ì˜ˆì‹œ:**
```bash
python step1_temporal_accumulation.py \
    /data/waymo/nre_format \
    --voxel_size 0.03 \
    --sample_interval 3
```

**ì¶œë ¥:**
- `step1_warped/`: ì‹œê³„ì—´ ëˆ„ì ìœ¼ë¡œ êµ¬ë©ì´ ë©”ì›Œì§„ ì´ë¯¸ì§€

### 3. Step 2: ê¸°í•˜í•™ì  ê°€ì´ë“œ ìƒì„± (Geometric Guide)

Step 1ì—ì„œ ì±„ì›Œì§€ì§€ ì•Šì€ êµ¬ë©ì„ RANSAC ê¸°ë°˜ í‰ë©´ í”¼íŒ…ìœ¼ë¡œ ê¸°í•˜í•™ì ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.

```bash
python step2_geometric_guide.py /path/to/preprocessing/output
```

**ì˜µì…˜:**
- `--no_lidar`: LiDAR depthë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  pseudo depth ìƒì„±
- `--ground_ratio`: ë°”ë‹¥ í‰ë©´ ì¶”ì •ì— ì‚¬ìš©í•  ì´ë¯¸ì§€ í•˜ë‹¨ ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.6)

**ì˜ˆì‹œ:**
```bash
python step2_geometric_guide.py \
    /data/waymo/nre_format \
    --ground_ratio 0.65
```

**ì¶œë ¥:**
- `step2_depth_guide/`: ê¸°í•˜í•™ì ìœ¼ë¡œ ì±„ì›Œì§„ depth guide maps
- `step2_hole_masks/`: ì±„ì›Œì•¼ í•  êµ¬ë© ì˜ì—­ ë§ˆìŠ¤í¬

### 4. Step 3: Multi-view Consistent ìµœì¢… ì¸í˜ì¸íŒ…

Step 1ê³¼ Step 2ì˜ ê²°ê³¼ë¥¼ ê²°í•©í•˜ì—¬ ìƒì„±í˜• AI ê¸°ë°˜ ìµœì¢… ì¸í˜ì¸íŒ…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

```bash
python step3_final_inpainting.py /path/to/preprocessing/output
```

**ì˜µì…˜:**
- `--use_ai`: ìƒì„±í˜• AI (Stable Diffusion) ì‚¬ìš© (ê¸°ë³¸ê°’: False, OpenCV inpainting)
- `--noise_level`: í…ìŠ¤ì²˜ ë…¸ì´ì¦ˆ ë ˆë²¨ (0-255, ê¸°ë³¸ê°’: 5)

**ì˜ˆì‹œ:**
```bash
# OpenCV ê¸°ë°˜ (ë¹ ë¥´ê³  ê°€ë²¼ì›€)
python step3_final_inpainting.py /data/waymo/nre_format

# Stable Diffusion ê¸°ë°˜ (ë” ìì—°ìŠ¤ëŸ¬ìš´ ê²°ê³¼)
python step3_final_inpainting.py /data/waymo/nre_format --use_ai
```

**ì¶œë ¥:**
- `step3_final_inpainted/`: ìµœì¢… ì¸í˜ì¸íŒ… ê²°ê³¼

### 5. Training Dataset Builder (ì„ íƒ)

ì¸í˜ì¸íŒ… ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ìƒì„±í˜• AI ëª¨ë¸ í•™ìŠµìš© ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.

```bash
# ëª¨ë“  ë°ì´í„°ì…‹ ìƒì„± (LoRA + ControlNet)
python training_dataset_builder.py /path/to/data --mode all

# LoRA ë°ì´í„°ì…‹ë§Œ
python training_dataset_builder.py /path/to/data --mode lora

# ControlNet Canny ë°ì´í„°ì…‹ë§Œ
python training_dataset_builder.py /path/to/data --mode controlnet_canny

# ControlNet Depth ë°ì´í„°ì…‹ë§Œ
python training_dataset_builder.py /path/to/data --mode controlnet_depth
```

**ì˜µì…˜:**
- `--output_dir`: ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: data_root/gen_ai_train)
- `--dynamic_threshold`: ë™ì  ê°ì²´ ë¹„ìœ¨ ì„ê³„ê°’ (0-1, ê¸°ë³¸ê°’: 0.05)
- `--max_samples`: ë°ì´í„°ì…‹ë‹¹ ìµœëŒ€ ìƒ˜í”Œ ìˆ˜
- `--lora_trigger`: LoRA íŠ¸ë¦¬ê±° ì›Œë“œ (ê¸°ë³¸ê°’: "WaymoStyle road")
- `--controlnet_prompt`: ControlNet í”„ë¡¬í”„íŠ¸
- `--use_original`: Step 3 ê²°ê³¼ ëŒ€ì‹  ì›ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©
- `--canny_low/high`: Canny edge detection ì„ê³„ê°’

**ì˜ˆì‹œ:**
```bash
python training_dataset_builder.py /data/waymo/nre_format \
    --mode all \
    --max_samples 1000 \
    --lora_trigger "WaymoStyle autonomous driving scene" \
    --dynamic_threshold 0.03
```

**ì¶œë ¥:**
- `gen_ai_train/lora_dataset/`: LoRA í•™ìŠµ ë°ì´í„°
  - `*.jpg`: ê¹¨ë—í•œ ë°°ê²½ ì´ë¯¸ì§€
  - `metadata.jsonl`: ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒ
- `gen_ai_train/controlnet_dataset/`: ControlNet í•™ìŠµ ë°ì´í„°
  - `train/`: Target ì´ë¯¸ì§€
  - `conditioning_images/`: Condition ì´ë¯¸ì§€ (Canny/Depth)
  - `metadata.jsonl`: ì´ë¯¸ì§€-condition-í…ìŠ¤íŠ¸ íŠ¸ë¦¬í”Œ

## ğŸ§  ì•Œê³ ë¦¬ì¦˜ ì„¤ëª…

### Step 1: Temporal Accumulation

**Forward Pass (ì •ì  í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ëˆ„ì ):**

1. ê° í”„ë ˆì„ì—ì„œ ì •ì  ì˜ì—­(mask=255)ì˜ í”½ì…€ë§Œ ì„ íƒ
2. Depth ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ 2D â†’ 3D backprojection
3. ì¹´ë©”ë¼ poseë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì—­ ì¢Œí‘œê³„ë¡œ ë³€í™˜
4. ëª¨ë“  í”„ë ˆì„ì˜ ì •ì  í¬ì¸íŠ¸ë¥¼ ëˆ„ì 
5. Voxel downsamplingìœ¼ë¡œ ì¤‘ë³µ ì œê±° ë° ë…¸ì´ì¦ˆ í•„í„°ë§

**Backward Pass (Reprojection):**

1. ì „ì—­ í¬ì¸íŠ¸ í´ë¼ìš°ë“œë¥¼ ê° í”„ë ˆì„ì˜ ì¹´ë©”ë¼ ì‹œì ìœ¼ë¡œ ë³€í™˜
2. 3D â†’ 2D projectionìœ¼ë¡œ ì´ë¯¸ì§€ í‰ë©´ì— ë Œë”ë§
3. Z-bufferingìœ¼ë¡œ ê°€ì‹œì„± ì²˜ë¦¬
4. ì‘ì€ êµ¬ë©ì€ OpenCV inpaintingìœ¼ë¡œ ì±„ì›€
5. ì›ë³¸ ì´ë¯¸ì§€ì™€ ë¸”ë Œë”© (ì •ì  ì˜ì—­ì€ ì›ë³¸ ìœ ì§€)

### Step 2: Geometric Guide Generation

**êµ¬ë© ê°ì§€ (Hole Detection):**

1. Step 1 ê²°ê³¼ì—ì„œ ê²€ì€ìƒ‰ í”½ì…€(ë°ê¸° < 10)ì„ êµ¬ë©ìœ¼ë¡œ ê°ì§€
2. Morphological closingìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°
3. ì‘ì€ êµ¬ë©(< 50 í”½ì…€)ì€ ë¬´ì‹œ

**í‰ë©´ í”¼íŒ… (RANSAC Plane Fitting):**

1. ì´ë¯¸ì§€ í•˜ë‹¨ 40% ì˜ì—­ì—ì„œ ë°”ë‹¥ í‰ë©´ ìƒ˜í”Œë§
2. RANSACìœ¼ë¡œ ì•„ì›ƒë¼ì´ì–´ì— ê°•ê±´í•œ í‰ë©´ ì¶”ì • (Z = aX + bY + c)
3. êµ¬ë© ì˜ì—­ì˜ depth ê°’ì„ í‰ë©´ ë°©ì •ì‹ìœ¼ë¡œ ì˜ˆì¸¡
4. ìŒìˆ˜ depth ê°’ í´ë¦¬í•‘ ë° ì •ê·œí™”

**Fallback ì „ëµ:**

- ìœ íš¨í•œ depth í¬ì¸íŠ¸ê°€ ë¶€ì¡±í•˜ë©´ OpenCV inpainting ì‚¬ìš©
- LiDAR depthê°€ ì—†ìœ¼ë©´ ì„ í˜• gradient pseudo depth ìƒì„±

### Step 3: Multi-view Consistent Final Inpainting

**Fusion Logic (ì´ë¯¸ì§€ ë³‘í•©):**

1. Step 1 warped ì´ë¯¸ì§€ì—ì„œ ê²€ì€ìƒ‰ í”½ì…€(êµ¬ë©) ê°ì§€
2. Step 2 hole maskì™€ ë³‘í•©í•˜ì—¬ ìµœì¢… êµ¬ë© ë§ˆìŠ¤í¬ ìƒì„±
3. ì›ë³¸ ì´ë¯¸ì§€ + Warped ì´ë¯¸ì§€ ìœµí•© (warped ìš°ì„ ìˆœìœ„ ë†’ìŒ)
4. Base ì´ë¯¸ì§€ ìƒì„±

**Inpainting (ìƒì„±í˜• AI):**

1. Base ì´ë¯¸ì§€ì™€ êµ¬ë© ë§ˆìŠ¤í¬ë¥¼ Stable Diffusionì— ì…ë ¥
2. Depth guideë¥¼ conditioningìœ¼ë¡œ ì‚¬ìš© (ControlNet)
3. Prompt: "realistic road surface, asphalt texture"
4. ìƒì„±ëœ ê²°ê³¼ì™€ ì›ë³¸ ë¸”ë Œë”©

**Texture Enhancement:**

1. ì¸í˜ì¸íŒ…ëœ ì˜ì—­ì— Gaussian noise ì¶”ê°€
2. ë¶€ë“œëŸ¬ìš´ ë¸”ë Œë”©ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ê²½ê³„
3. Sim-to-Real í…ìŠ¤ì²˜ ê°­ ì™„í™”

## ğŸ“Š ì…ì¶œë ¥ ë°ì´í„° í¬ë§·

### ì…ë ¥: Preprocessing Output

**ë””ë ‰í† ë¦¬ êµ¬ì¡°:**
```
data_root/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ seq0_000001_FRONT.jpg
â”‚   â”œâ”€â”€ seq0_000001_FRONT_LEFT.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ masks/
â”‚   â”œâ”€â”€ seq0_000001_FRONT.png      # 0=ë™ì , 255=ì •ì 
â”‚   â””â”€â”€ ...
â”œâ”€â”€ poses/
â”‚   â”œâ”€â”€ seq0_000001.json
â”‚   â””â”€â”€ ...
â””â”€â”€ depths/  (ì„ íƒ)
    â”œâ”€â”€ seq0_000001_FRONT.png      # uint16, mm ë‹¨ìœ„
    â””â”€â”€ ...
```

**Pose JSON í¬ë§·:**
```json
{
    "frame_idx": 1,
    "timestamp": 1234567890.123456,
    "cameras": {
        "FRONT": {
            "img_path": "images/seq0_000001_FRONT.jpg",
            "width": 1920,
            "height": 1280,
            "intrinsics": [fx, fy, cx, cy, k1, k2, p1, p2, k3],
            "pose": [...]  // 4x4 matrix (flatten)
        }
    }
}
```

### ì¶œë ¥: Step 1 Warped Images

```
data_root/
â””â”€â”€ step1_warped/
    â”œâ”€â”€ seq0_000001_FRONT.png
    â”œâ”€â”€ seq0_000001_FRONT_LEFT.png
    â””â”€â”€ ...
```

### ì¶œë ¥: Step 2 Geometric Guides

```
data_root/
â”œâ”€â”€ step2_depth_guide/
â”‚   â”œâ”€â”€ seq0_000001_FRONT.png      # ê¸°í•˜í•™ì ìœ¼ë¡œ ì±„ì›Œì§„ depth
â”‚   â””â”€â”€ ...
â””â”€â”€ step2_hole_masks/
    â”œâ”€â”€ seq0_000001_FRONT.png      # 255=êµ¬ë©, 0=ì±„ì›Œì§
    â””â”€â”€ ...
```

### ì¶œë ¥: Step 3 Final Inpainted Images

```
data_root/
â””â”€â”€ step3_final_inpainted/
    â”œâ”€â”€ seq0_000001_FRONT.png      # ìµœì¢… ì™„ì„±ëœ ì´ë¯¸ì§€
    â”œâ”€â”€ seq0_000001_FRONT_LEFT.png
    â””â”€â”€ ...
```

### ì¶œë ¥: Training Datasets

```
gen_ai_train/
â”œâ”€â”€ lora_dataset/
â”‚   â”œâ”€â”€ 000000.jpg                 # ê¹¨ë—í•œ ë°°ê²½ ì´ë¯¸ì§€
â”‚   â”œâ”€â”€ 000001.jpg
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ metadata.jsonl             # HuggingFace format
â”‚
â””â”€â”€ controlnet_dataset/
    â”œâ”€â”€ train/
    â”‚   â”œâ”€â”€ 000000.jpg             # Target ì´ë¯¸ì§€
    â”‚   â”œâ”€â”€ 000001.jpg
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ conditioning_images/
    â”‚   â”œâ”€â”€ 000000_cond.png        # Canny edge or Depth
    â”‚   â”œâ”€â”€ 000001_cond.png
    â”‚   â””â”€â”€ ...
    â””â”€â”€ metadata.jsonl             # HuggingFace format
```

**metadata.jsonl í¬ë§·:**

LoRA:
```json
{"file_name": "000000.jpg", "text": "WaymoStyle road", "original_file": "seq0_000001_FRONT.jpg"}
```

ControlNet:
```json
{"text": "high quality road scene", "image": "train/000000.jpg", "conditioning_image": "conditioning_images/000000_cond.png", "original_file": "seq0_000001_FRONT.jpg"}
```

## ğŸ”§ ì˜ì¡´ì„±

```bash
pip install opencv-python numpy open3d tqdm scikit-learn
```

**í•„ìˆ˜:**
- `opencv-python`: ì´ë¯¸ì§€ ì²˜ë¦¬
- `numpy`: ìˆ˜ì¹˜ ì—°ì‚°
- `open3d`: 3D í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ì²˜ë¦¬ (Step 1)
- `scikit-learn`: RANSAC íšŒê·€ (Step 2)
- `tqdm`: í”„ë¡œê·¸ë ˆìŠ¤ ë°”

## âš™ï¸ ê³ ê¸‰ ì„¤ì •

### Depth ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°

`step1_temporal_accumulation.py`ëŠ” depth ë§µì´ ì—†ìœ¼ë©´ pseudo depth(ê³ ì • ê±°ë¦¬)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ìœ„í•´ ë‹¨ì•ˆ depth estimationì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
# TODO: Monocular depth estimation í†µí•©
# from depth_anything import DepthEstimator
# depth_estimator = DepthEstimator()
# depth = depth_estimator.predict(image)
```

### ìƒì„±í˜• AI í†µí•©

`step3_final_inpainting.py`ì—ì„œ Stable Diffusionì„ ì‚¬ìš©í•˜ë ¤ë©´:

```python
# step3_final_inpainting.py ë‚´ë¶€ _initialize_generative_model() ìˆ˜ì •
from diffusers import StableDiffusionInpaintPipeline
import torch

model_id = "stabilityai/stable-diffusion-2-inpainting"
self.pipe = StableDiffusionInpaintPipeline.from_pretrained(
    model_id,
    torch_dtype=torch.float16
)
self.pipe = self.pipe.to("cuda")
```

**ì‹¤í–‰:**
```bash
pip install diffusers transformers accelerate
python step3_final_inpainting.py /data/waymo/nre_format --use_ai
```

### í•™ìŠµ ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ í•™ìŠµ

ìƒì„±ëœ ë°ì´í„°ì…‹ìœ¼ë¡œ HuggingFace Diffusers í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©:

**LoRA í•™ìŠµ:**
```bash
# HuggingFace diffusers ì„¤ì¹˜
pip install diffusers transformers accelerate

# LoRA í•™ìŠµ
python train_text_to_image_lora.py \
    --pretrained_model_name_or_path="runwayml/stable-diffusion-v1-5" \
    --train_data_dir="gen_ai_train/lora_dataset" \
    --caption_column="text" \
    --resolution=512 \
    --train_batch_size=4 \
    --num_train_epochs=100 \
    --learning_rate=1e-4 \
    --lr_scheduler="constant" \
    --lr_warmup_steps=0 \
    --output_dir="./output/waymo_lora"
```

**ControlNet í•™ìŠµ:**
```bash
# ControlNet í•™ìŠµ
python train_controlnet.py \
    --pretrained_model_name_or_path="runwayml/stable-diffusion-v1-5" \
    --train_data_dir="gen_ai_train/controlnet_dataset" \
    --conditioning_image_column="conditioning_image" \
    --image_column="image" \
    --caption_column="text" \
    --resolution=512 \
    --train_batch_size=4 \
    --num_train_epochs=100 \
    --learning_rate=1e-5 \
    --output_dir="./output/waymo_controlnet"
```

### ë©”ëª¨ë¦¬ ìµœì í™”

ëŒ€ìš©ëŸ‰ ì‹œí€€ìŠ¤ ì²˜ë¦¬ ì‹œ ë©”ëª¨ë¦¬ ë¶€ì¡±ì´ ë°œìƒí•˜ë©´:

1. `--sample_interval` ì¦ê°€ (Forward pass ìƒ˜í”Œë§ ê°„ê²©)
2. `--voxel_size` ì¦ê°€ (í¬ì¸íŠ¸ í´ë¼ìš°ë“œ í•´ìƒë„ ê°ì†Œ)
3. ì‹œí€€ìŠ¤ë¥¼ ì‘ì€ ì²­í¬ë¡œ ë¶„í• 

```bash
# ì˜ˆ: 10í”„ë ˆì„ë§ˆë‹¤ ìƒ˜í”Œë§, voxel í¬ê¸° 10cm
python step1_temporal_accumulation.py /data/waymo/nre_format \
    --sample_interval 10 \
    --voxel_size 0.1
```

## ğŸš€ ì „ì²´ íŒŒì´í”„ë¼ì¸

ì™„ì „í•œ ì¸í˜ì¸íŒ… + í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„± íŒŒì´í”„ë¼ì¸:

```bash
# 0. Preprocessing
cd preprocessing
python waymo2nre.py /path/to/waymo/raw /path/to/output

# 1. ì‹œê³„ì—´ ëˆ„ì 
cd ../Inpainting
python step1_temporal_accumulation.py /path/to/output

# 2. ê¸°í•˜í•™ì  ê°€ì´ë“œ
python step2_geometric_guide.py /path/to/output

# 3. ìµœì¢… ì¸í˜ì¸íŒ…
python step3_final_inpainting.py /path/to/output --use_ai

# 4. í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„± (ì„ íƒ)
python training_dataset_builder.py /path/to/output --mode all
```

**ê²°ê³¼:**
- `step3_final_inpainted/`: ë™ì  ê°ì²´ê°€ ì œê±°ë˜ê³  ì •ì  ë°°ê²½ìœ¼ë¡œ ì±„ì›Œì§„ ì™„ì„± ì´ë¯¸ì§€
- `gen_ai_train/`: ìƒì„±í˜• AI ëª¨ë¸ í•™ìŠµìš© ë°ì´í„°ì…‹ (LoRA + ControlNet)

## ğŸ“ ì°¸ê³ ì‚¬í•­

- **Pose ì •í™•ë„**: ì‹œê³„ì—´ ëˆ„ì ì˜ í’ˆì§ˆì€ pose ì •í™•ë„ì— í¬ê²Œ ì˜ì¡´í•©ë‹ˆë‹¤
- **ì •ì  ê°€ì •**: ë°°ê²½ì´ ì •ì ì´ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤ (ì›€ì§ì´ëŠ” ë‚˜ë­‡ì, ë¬¼ ë“±ì€ artifacts ë°œìƒ ê°€ëŠ¥)
- **ì‹œì  ë³€í™”**: ê¸‰ê²©í•œ ì‹œì  ë³€í™”ê°€ ìˆìœ¼ë©´ ëˆ„ì  íš¨ê³¼ê°€ ê°ì†Œí•©ë‹ˆë‹¤

## ğŸ› ë¬¸ì œ í•´ê²°

### "No points accumulated" ê²½ê³  (Step 1)

**ì›ì¸:**
- Depth íŒŒì¼ì´ ì—†ê±°ë‚˜ ê²½ë¡œê°€ ì˜ëª»ë¨
- Maskê°€ ëª¨ë‘ 0 (ë™ì )ìœ¼ë¡œ ë˜ì–´ ìˆìŒ
- Pose íŒŒì¼ í˜•ì‹ì´ ë‹¤ë¦„

**í•´ê²°:**
1. Depth íŒŒì¼ ê²½ë¡œ í™•ì¸: `data_root/depths/`
2. Mask í™•ì¸: `cv2.imread(mask_path)`ë¡œ ë¡œë“œí–ˆì„ ë•Œ 255 ê°’ì´ ìˆëŠ”ì§€ í™•ì¸
3. Pose JSON êµ¬ì¡° í™•ì¸

### "Insufficient valid depth points" ê²½ê³  (Step 2)

**ì›ì¸:**
- Step 1 ê²°ê³¼ì— ìœ íš¨í•œ depth í¬ì¸íŠ¸ê°€ ë„ˆë¬´ ì ìŒ
- ë°”ë‹¥ í‰ë©´ ì¶”ì •ì„ ìœ„í•œ ìƒ˜í”Œì´ ë¶€ì¡±í•¨

**í•´ê²°:**
1. `--ground_ratio` ê°’ì„ ì¡°ì • (ì˜ˆ: 0.5ë¡œ ë‚®ì¶”ë©´ ë” ë§ì€ ì˜ì—­ ì‚¬ìš©)
2. Step 1ì˜ voxel_sizeë¥¼ ì¤„ì—¬ ë” ì¡°ë°€í•œ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ìƒì„±
3. `--no_lidar` ì˜µì…˜ ì‚¬ìš©í•˜ì—¬ pseudo depthë¡œ ëŒ€ì²´

### Open3D ì˜¤ë¥˜

```bash
# Open3Dê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë²„ì „ ë¬¸ì œ
pip install --upgrade open3d
```

### ë©”ëª¨ë¦¬ ë¶€ì¡±

```bash
# CUDA out of memory (Open3D CUDA ì‚¬ìš© ì‹œ)
export OPEN3D_CPU_RENDERING=1

# ë˜ëŠ” sample_interval ì¦ê°€
python step1_temporal_accumulation.py /data --sample_interval 10
```

## ğŸ“„ ë¼ì´ì„¼ìŠ¤

Photo-real_projectì™€ ë™ì¼í•œ ë¼ì´ì„¼ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤.
