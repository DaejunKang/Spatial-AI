# Photo-real Project: Complete Workflow Guide

**Waymo Open Dataset â†’ ë™ì  ê°ì²´ ì œê±° â†’ ê¹¨ë—í•œ ë°°ê²½ ì´ë¯¸ì§€ ìƒì„±**

ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ í†µí•´ ììœ¨ì£¼í–‰ ë°ì´í„°ì˜ ë™ì  ê°ì²´ë¥¼ ì œê±°í•˜ê³ , NeRF/3DGS í•™ìŠµìš© ê³ í’ˆì§ˆ ë°°ê²½ ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.

---

## ğŸ“‹ ëª©ì°¨

1. [ì „ì²´ ì›Œí¬í”Œë¡œìš°](#ì „ì²´-ì›Œí¬í”Œë¡œìš°)
2. [Stageë³„ Input/Output ì¸í„°í˜ì´ìŠ¤](#stageë³„-inputoutput-ì¸í„°í˜ì´ìŠ¤)
3. [ë¹ ë¥¸ ì‹œì‘](#ë¹ ë¥¸-ì‹œì‘)
4. [ìƒì„¸ ê°€ì´ë“œ](#ìƒì„¸-ê°€ì´ë“œ)
5. [ë””ë ‰í† ë¦¬ êµ¬ì¡°](#ë””ë ‰í† ë¦¬-êµ¬ì¡°)

---

## ğŸ”„ ì „ì²´ ì›Œí¬í”Œë¡œìš°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Waymo Open Dataset (Raw)                            â”‚
â”‚                       .tfrecord files                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 1: PARSING                                                      â”‚
â”‚  â”œâ”€ parsing/waymo2nre.py                                              â”‚
â”‚  â”œâ”€ Input: .tfrecord                                                  â”‚
â”‚  â””â”€ Output: images/, point_clouds/, poses/, objects/                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 2: PREPROCESSING                                                â”‚
â”‚  â”œâ”€ preprocessing/lidar_projection.py (LiDAR â†’ Image)                â”‚
â”‚  â”œâ”€ preprocessing/dynamic_masking.py (3D Box â†’ 2D Mask)              â”‚
â”‚  â”œâ”€ Input: images/, point_clouds/, poses/, objects/                  â”‚
â”‚  â””â”€ Output: depth_maps/, masks/                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 3: INPAINTING (Two Approaches)                                 â”‚
â”‚                                                                        â”‚
â”‚  Approach 1: COLMAP-based                                             â”‚
â”‚  â”œâ”€ Inpainting/approach1_colmap.py                                   â”‚
â”‚  â”œâ”€ SfM + MVS â†’ 3D Reconstruction                                    â”‚
â”‚  â””â”€ Output: final_inpainted/                                         â”‚
â”‚                                                                        â”‚
â”‚  Approach 2: Sequential (ê¶Œì¥)                                         â”‚
â”‚  â”œâ”€ Inpainting/approach2_sequential.py                               â”‚
â”‚  â”œâ”€ Step 1: Temporal Accumulation                                    â”‚
â”‚  â”œâ”€ Step 2: Geometric Guide                                          â”‚
â”‚  â”œâ”€ Step 3: AI Inpainting (SD + ControlNet + LoRA)                  â”‚
â”‚  â””â”€ Output: final_inpainted/                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Final Output: Clean Background Images                                â”‚
â”‚  â†’ NeRF/3DGS Training                                                 â”‚
â”‚  â†’ Simulation Environment                                             â”‚
â”‚  â†’ Data Augmentation                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Stageë³„ Input/Output ì¸í„°í˜ì´ìŠ¤

### Stage 1: Parsing (ë°ì´í„° íŒŒì‹±)

**ìŠ¤í¬ë¦½íŠ¸:** `parsing/waymo2nre.py`

#### Input
```
waymo_raw/
â””â”€â”€ segment-XXXXX.tfrecord  # Waymo ë°”ì´ë„ˆë¦¬ ë°ì´í„°
```

#### Process
- TFRecord íŒŒì‹± (No TensorFlow)
- ì¢Œí‘œê³„ ì •ê·œí™” (ì²« í”„ë ˆì„ = Origin)
- ì´ë¯¸ì§€, LiDAR, Pose, Objects ì¶”ì¶œ
- Rolling Shutter ì •ë³´ ì¶”ì¶œ

#### Output
```
nre_format/
â”œâ”€â”€ images/
â”‚   â””â”€â”€ {prefix}{file_idx:03d}{frame_idx:03d}_{cam_name}.jpg
â”‚       # ì›ë³¸ ì´ë¯¸ì§€ (5 cameras: FRONT, FRONT_LEFT, FRONT_RIGHT, SIDE_LEFT, SIDE_RIGHT)
â”‚
â”œâ”€â”€ point_clouds/
â”‚   â””â”€â”€ {prefix}{file_idx:03d}{frame_idx:03d}.bin
â”‚       # LiDAR í¬ì¸íŠ¸ í´ë¼ìš°ë“œ (Nx3 float32, Local World ì¢Œí‘œ)
â”‚
â”œâ”€â”€ poses/
â”‚   â””â”€â”€ {prefix}{file_idx:03d}{frame_idx:03d}.json
â”‚       # í”„ë ˆì„ë³„ ë©”íƒ€ë°ì´í„°
â”‚       {
â”‚         "timestamp": float,
â”‚         "ego_velocity": {"linear": [vx,vy,vz], "angular": [wx,wy,wz]},
â”‚         "cameras": {
â”‚           "FRONT": {
â”‚             "img_path": str,
â”‚             "width": int, "height": int,
â”‚             "intrinsics": [fx, fy, cx, cy, k1, k2, p1, p2, k3],
â”‚             "pose": [4x4 matrix],
â”‚             "rolling_shutter": {"duration": float, "trigger_time": float}
â”‚           },
â”‚           ...
â”‚         }
â”‚       }
â”‚
â””â”€â”€ objects/
    â””â”€â”€ {prefix}{file_idx:03d}{frame_idx:03d}.json
        # ë™ì  ê°ì²´ ì •ë³´
        [
          {
            "id": str,
            "class": "TYPE_VEHICLE" | "TYPE_PEDESTRIAN" | "TYPE_CYCLIST",
            "box": {
              "center": [x, y, z],  # Local World
              "size": [length, width, height],
              "heading": float  # rad
            },
            "speed": [vx, vy]
          },
          ...
        ]
```

**ì‹¤í–‰:**
```bash
python parsing/waymo2nre.py \
    /path/to/waymo_raw \
    /path/to/nre_format \
    --prefix seq0_
```

**ìƒì„¸ ë¬¸ì„œ:** [parsing/README.md](parsing/README.md)

---

### Stage 2: Preprocessing (ì „ì²˜ë¦¬)

**ìŠ¤í¬ë¦½íŠ¸:** `preprocessing/run_preprocessing.py`

#### Input
```
nre_format/  # Stage 1 ì¶œë ¥
â”œâ”€â”€ images/
â”œâ”€â”€ point_clouds/
â”œâ”€â”€ poses/
â””â”€â”€ objects/
```

#### Process

**2.1 LiDAR Projection** (`lidar_projection.py`)
- LiDAR 3D í¬ì¸íŠ¸ â†’ ë‹¤ì¤‘ ë·° ì´ë¯¸ì§€ íˆ¬ì˜
- íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ì‹œê³µê°„ ë™ê¸°í™”
- ê¹Šì´ ë§µ ìƒì„± (ë³´ê°„ í¬í•¨)

**2.2 Dynamic Masking** (`dynamic_masking.py`)
- 3D Bounding Box â†’ 2D Convex Hull íˆ¬ì˜
- Semantic Segmentation í†µí•© (ì„ íƒ)
- ì•ˆì „ ë§ˆì§„ ì¶”ê°€ (Dilation)

#### Output
```
nre_format/
â”œâ”€â”€ [Stage 1 outputs...]
â”‚
â”œâ”€â”€ depth_maps/
â”‚   â”œâ”€â”€ FRONT/
â”‚   â”‚   â””â”€â”€ {frame}.png  # uint16, mm ë‹¨ìœ„
â”‚   â”œâ”€â”€ FRONT_LEFT/
â”‚   â””â”€â”€ ...
â”‚       # LiDAR íˆ¬ì˜ ê¹Šì´ ë§µ (Inpainting Step 2 & 3ì—ì„œ ì‚¬ìš©)
â”‚
â”œâ”€â”€ point_masks/
â”‚   â””â”€â”€ {cam_name}/{frame}.png  # uint8, LiDAR í¬ì¸íŠ¸ ì‹œê°í™”
â”‚
â””â”€â”€ masks/
    â”œâ”€â”€ FRONT/
    â”‚   â””â”€â”€ {frame}.png  # uint8
    â”‚       # 0 = ë™ì  ê°ì²´ (Inpainting ëŒ€ìƒ)
    â”‚       # 255 = ì •ì  ë°°ê²½ (ìœ íš¨ ì˜ì—­)
    â”œâ”€â”€ FRONT_LEFT/
    â””â”€â”€ ...
```

**ì‹¤í–‰:**
```bash
# ì „ì²´ ì‹¤í–‰
python preprocessing/run_preprocessing.py \
    /path/to/nre_format \
    --all

# ë‹¨ê³„ë³„ ì‹¤í–‰
python preprocessing/run_preprocessing.py \
    /path/to/nre_format \
    --lidar \           # LiDAR íˆ¬ì˜
    --dynamic_mask \    # ë™ì  ê°ì²´ ë§ˆìŠ¤í‚¹
    --semantic          # Semantic Segmentation í¬í•¨ (ì„ íƒ)
```

**ìƒì„¸ ë¬¸ì„œ:** [preprocessing/README.md](preprocessing/README.md)

---

### Stage 3: Inpainting (ì¸í˜ì¸íŒ…)

#### Input
```
nre_format/  # Stage 1 & 2 ì¶œë ¥
â”œâ”€â”€ images/           # ì›ë³¸ ì´ë¯¸ì§€
â”œâ”€â”€ masks/            # ë™ì  ê°ì²´ ë§ˆìŠ¤í¬
â”œâ”€â”€ poses/            # ì¹´ë©”ë¼ í¬ì¦ˆ
â”œâ”€â”€ depth_maps/       # LiDAR ê¹Šì´
â””â”€â”€ objects/          # ë™ì  ê°ì²´ ì •ë³´
```

---

#### Approach 1: COLMAP-based Scene Reconstruction

**ìŠ¤í¬ë¦½íŠ¸:** `Inpainting/approach1_colmap.py`

**ì „ëµ:** 3D ì¬êµ¬ì„± ê¸°ë°˜ ê³µê°„ì  ì¼ê´€ì„±

**Process:**
1. Feature Extraction (ì •ì  ì˜ì—­ë§Œ)
2. Feature Matching (Sequential)
3. SfM (Structure from Motion)
4. MVS (Multi-View Stereo)
5. Hole Filling (Novel View Synthesis)
6. Post-processing

**ì¤‘ê°„ ì¶œë ¥:**
```
nre_format/colmap_workspace/
â”œâ”€â”€ database.db           # COLMAP ë°ì´í„°ë² ì´ìŠ¤
â”œâ”€â”€ sparse/0/
â”‚   â”œâ”€â”€ cameras.bin       # ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„°
â”‚   â”œâ”€â”€ images.bin        # í¬ì¦ˆ
â”‚   â””â”€â”€ points3D.bin      # Sparse 3D
â””â”€â”€ dense/
    â”œâ”€â”€ fused.ply         # Dense 3D
    â””â”€â”€ stereo/depth_maps/
        â””â”€â”€ *.bin         # Depth maps
```

**ì‹¤í–‰:**
```bash
python Inpainting/approach1_colmap.py \
    /path/to/nre_format \
    --colmap_path colmap
```

**íŠ¹ì§•:**
- âœ… Multi-view consistency ìë™ ë³´ì¥
- âœ… ê¸°í•˜í•™ì  ì •í™•ë„ ë†’ìŒ
- âš ï¸ ëŠë¦¼ (~1-5ì‹œê°„)
- âš ï¸ COLMAP ì™¸ë¶€ ì˜ì¡´ì„±

---

#### Approach 2: Sequential Multi-Stage Pipeline (ê¶Œì¥)

**ìŠ¤í¬ë¦½íŠ¸:** `Inpainting/approach2_sequential.py`

**ì „ëµ:** ì‹œê³„ì—´ ëˆ„ì  â†’ ê¸°í•˜í•™ì  ê°€ì´ë“œ â†’ AI ìƒì„±

**Process:**

**Step 1: Temporal Accumulation** (`step1_temporal_accumulation.py`)
- ì—¬ëŸ¬ í”„ë ˆì„ì˜ ì •ì  ë°°ê²½ì„ 3Dë¡œ ëˆ„ì 
- Voxel downsampling (5cm)
- ê¸°ì¤€ í”„ë ˆì„ì— ì¬íˆ¬ì˜

**ì¤‘ê°„ ì¶œë ¥:**
```
nre_format/step1_warped/
â””â”€â”€ *.png  # ì‹œê³„ì—´ ëˆ„ì  ê²°ê³¼ (70-85% ë³µì›)
```

---

**Step 2: Geometric Guide** (`step2_geometric_guide.py`)
- Step 1 ì‹¤íŒ¨ ì˜ì—­ íƒì§€
- RANSAC í‰ë©´ ì¶”ì •
- êµ¬ë© ì˜ì—­ depth ì˜ˆì¸¡

**ì¤‘ê°„ ì¶œë ¥:**
```
nre_format/
â”œâ”€â”€ step2_depth_guide/
â”‚   â””â”€â”€ *.png  # uint16, ê¸°í•˜í•™ì  depth ê°€ì´ë“œ
â””â”€â”€ step2_hole_masks/
    â””â”€â”€ *.png  # uint8, ìµœì¢… êµ¬ë© ë§ˆìŠ¤í¬
```

---

**Step 3: AI Inpainting** (`step3_final_inpainting.py`)
- Stable Diffusion 1.5
- ControlNet (Depth)
- LoRA (Waymo íŠ¹í™”, ì„ íƒ)

**ëª¨ë¸:**
- Stable Diffusion 1.5: 4GB (ì‚¬ì „í•™ìŠµ)
- ControlNet Depth: 1.5GB (ì‚¬ì „í•™ìŠµ)
- LoRA: 10MB (ì„ íƒì  í•™ìŠµ)

**ì¤‘ê°„ ì¶œë ¥:**
```
nre_format/step3_final_inpainted/
â””â”€â”€ *.jpg  # AI ìƒì„± ìµœì¢… ê²°ê³¼
```

---

**ì‹¤í–‰:**
```bash
# ì „ì²´ íŒŒì´í”„ë¼ì¸
python Inpainting/approach2_sequential.py \
    /path/to/nre_format \
    --voxel_size 0.05 \
    --sample_interval 5 \
    --ground_ratio 0.6 \
    --lora_path ./trained_lora.safetensors  # ì„ íƒ

# ë‹¨ê³„ë³„ ì‹¤í–‰
python Inpainting/step1_temporal_accumulation.py --data_root /path/to/nre_format
python Inpainting/step2_geometric_guide.py --data_root /path/to/nre_format
python Inpainting/step3_final_inpainting.py --data_root /path/to/nre_format
```

**íŠ¹ì§•:**
- âœ… ë¹ ë¦„ (~10-30ë¶„)
- âœ… ê³ í’ˆì§ˆ í…ìŠ¤ì²˜
- âœ… 100% ì™„ì „ì„±
- âœ… LoRA ë„ë©”ì¸ íŠ¹í™”
- âš ï¸ GPU í•„ìˆ˜

---

#### ìµœì¢… Output (ê³µí†µ)
```
nre_format/final_inpainted/
â””â”€â”€ *.jpg
    # ë™ì  ê°ì²´ê°€ ì™„ì „íˆ ì œê±°ëœ ê¹¨ë—í•œ ë°°ê²½ ì´ë¯¸ì§€
    # NeRF/3DGS í•™ìŠµ, Simulation, Data Augmentationì— í™œìš©
```

**ìƒì„¸ ë¬¸ì„œ:** [Inpainting/README.md](Inpainting/README.md)

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### ì „ì²´ íŒŒì´í”„ë¼ì¸ í•œ ë²ˆì— ì‹¤í–‰

```bash
#!/bin/bash
# complete_pipeline.sh

DATA_ROOT="/path/to/nre_format"
WAYMO_RAW="/path/to/waymo_raw"

# Stage 1: Parsing
python parsing/waymo2nre.py \
    $WAYMO_RAW \
    $DATA_ROOT \
    --prefix seq0_

# Stage 2: Preprocessing
python preprocessing/run_preprocessing.py \
    $DATA_ROOT \
    --all

# Stage 3: Inpainting (Approach 2)
python Inpainting/approach2_sequential.py \
    $DATA_ROOT \
    --voxel_size 0.05 \
    --sample_interval 5

echo "Pipeline complete! Check $DATA_ROOT/final_inpainted/"
```

---

## ğŸ“– ìƒì„¸ ê°€ì´ë“œ

### í™˜ê²½ ì„¤ì •

```bash
# ê¸°ë³¸ íŒ¨í‚¤ì§€
pip install numpy opencv-python tqdm pillow

# Waymo
pip install waymo-open-dataset-tf-2-11-0

# Preprocessing
pip install open3d scikit-learn

# Inpainting (Approach 2)
pip install torch torchvision diffusers transformers accelerate

# COLMAP (Approach 1)
sudo apt-get install colmap  # Ubuntu
brew install colmap          # macOS
```

---

### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (Approach 2 Step 3)

**ìë™ ë‹¤ìš´ë¡œë“œ (ê¶Œì¥):**
```python
# ì²« ì‹¤í–‰ ì‹œ ìë™ ë‹¤ìš´ë¡œë“œë¨ (~15-20ë¶„)
python Inpainting/approach2_sequential.py /path/to/data
```

**ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ:**
```python
from diffusers import StableDiffusionControlNetInpaintPipeline, ControlNetModel

# SD 1.5 (~4GB)
pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16
)

# ControlNet Depth (~1.5GB)
controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/control_v11f1p_sd15_depth",
    torch_dtype=torch.float16
)
```

---

### LoRA í•™ìŠµ (ì„ íƒì )

```bash
# ë°©ë²• 1: í†µí•© í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (ê¶Œì¥)
python Inpainting/train_style_lora.py \
    --data_root /path/to/waymo_nre_format \
    --output_dir ./lora_output \
    --trigger_word "WaymoStyle road" \
    --max_train_steps 1000 \
    --lora_rank 16

# ë°©ë²• 2: Gradio UI ì‚¬ìš©
python Inpainting/lora_ui.py --port 7860
# â†’ ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:7860 ì ‘ì†

# í•™ìŠµëœ LoRA ì‚¬ìš©
python Inpainting/approach2_sequential.py \
    /path/to/data \
    --lora_path ./lora_output/pytorch_lora_weights.safetensors

# LoRA ì „/í›„ ë¹„êµ
python Inpainting/lora_inference.py compare \
    --lora_path ./lora_output/pytorch_lora_weights.safetensors \
    --output_dir ./comparison
```

---

## ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
Photo-real_project/
â”‚
â”œâ”€â”€ parsing/                      # Stage 1: Parsing
â”‚   â”œâ”€â”€ waymo2nre.py              # Waymo â†’ NRE ë³€í™˜ (ê¶Œì¥)
â”‚   â”œâ”€â”€ extract_waymo_data.py     # TensorFlow ë²„ì „
â”‚   â”œâ”€â”€ extract_waymo_data_minimal.py  # TF ì œê±° ë²„ì „
â”‚   â”œâ”€â”€ waymo_utils.py            # ê³µí†µ ìœ í‹¸ë¦¬í‹°
â”‚   â””â”€â”€ README.md                 # ìƒì„¸ ê°€ì´ë“œ
â”‚
â”œâ”€â”€ preprocessing/                # Stage 2: Preprocessing
â”‚   â”œâ”€â”€ lidar_projection.py       # LiDAR â†’ Image íˆ¬ì˜
â”‚   â”œâ”€â”€ dynamic_masking.py        # ë™ì  ê°ì²´ ë§ˆìŠ¤í‚¹
â”‚   â”œâ”€â”€ segmentation.py           # Semantic Segmentation
â”‚   â”œâ”€â”€ run_preprocessing.py      # í†µí•© ì‹¤í–‰
â”‚   â””â”€â”€ README.md                 # ìƒì„¸ ê°€ì´ë“œ
â”‚
â”œâ”€â”€ Inpainting/                   # Stage 3: Inpainting
â”‚   â”œâ”€â”€ approach1_colmap.py       # COLMAP ê¸°ë°˜ (3D ì¬êµ¬ì„±)
â”‚   â”œâ”€â”€ approach2_sequential.py   # Sequential í†µí•© ì‹¤í–‰
â”‚   â”œâ”€â”€ step1_temporal_accumulation.py  # ì‹œê³„ì—´ ëˆ„ì 
â”‚   â”œâ”€â”€ step2_geometric_guide.py        # ê¸°í•˜í•™ì  ê°€ì´ë“œ
â”‚   â”œâ”€â”€ step3_final_inpainting.py       # AI ìµœì¢… ìƒì„±
â”‚   â”œâ”€â”€ training_dataset_builder.py     # LoRA í•™ìŠµ ë°ì´í„°
â”‚   â”œâ”€â”€ train_style_lora.py       # Style LoRA í•™ìŠµ íŒŒì´í”„ë¼ì¸
â”‚   â”œâ”€â”€ lora_inference.py         # LoRA ì¶”ë¡  & í’ˆì§ˆ í‰ê°€
â”‚   â”œâ”€â”€ lora_ui.py                # Gradio ê¸°ë°˜ í†µí•© UI
â”‚   â””â”€â”€ README.md                 # ìƒì„¸ ê°€ì´ë“œ (ëª¨ë¸ ì •ë³´ í¬í•¨)
â”‚
â”œâ”€â”€ download/
â”‚   â””â”€â”€ download_waymo.py         # Waymo ë‹¤ìš´ë¡œë“œ
â”‚
â”œâ”€â”€ dataset.py                    # ë°ì´í„°ì…‹ ê´€ë¦¬
â”œâ”€â”€ reconstruction.py             # 3D ì¬êµ¬ì„± (USD ë³€í™˜)
â””â”€â”€ README.md                     # ğŸ“– ì´ ë¬¸ì„œ
```

---

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ

| Stage | Approach | ì²˜ë¦¬ ì‹œê°„ (100 frames) | ë©”ëª¨ë¦¬ | íŠ¹ì§• |
|-------|----------|----------------------|--------|------|
| **Parsing** | waymo2nre | ~5ë¶„ | 2GB RAM | TF ë¶ˆí•„ìš” |
| **Preprocessing** | LiDAR + Masking | ~10ë¶„ | 4GB RAM | GPU ì„ íƒ |
| **Inpainting** | COLMAP | 1-5ì‹œê°„ | 8-16GB RAM | Multi-view ì¼ê´€ì„± |
| **Inpainting** | Sequential | 10-30ë¶„ | 6GB VRAM | ë¹ ë¥´ê³  ê³ í’ˆì§ˆ |

---

## ğŸ¯ ì‚¬ìš© ì‚¬ë¡€ë³„ ê¶Œì¥ Workflow

### 1. NeRF/3DGS í•™ìŠµìš© ë°ì´í„° ì¤€ë¹„
```bash
# Approach 2 Sequential (ë¹ ë¥´ê³  ê³ í’ˆì§ˆ)
parsing/waymo2nre.py â†’ preprocessing/run_preprocessing.py â†’ Inpainting/approach2_sequential.py
```
**ì´ìœ :** ë¹ ë¥¸ ì²˜ë¦¬, ê³ í’ˆì§ˆ í…ìŠ¤ì²˜, 2D ì´ë¯¸ì§€ë§Œ í•„ìš”

---

### 2. 3D ì¬êµ¬ì„± + Novel View Synthesis
```bash
# Approach 1 COLMAP (ê¸°í•˜í•™ì  ì •í™•ë„)
parsing/waymo2nre.py â†’ preprocessing/run_preprocessing.py â†’ Inpainting/approach1_colmap.py
```
**ì´ìœ :** Multi-view consistency, 3D ëª¨ë¸ í™œìš© ê°€ëŠ¥

---

### 3. ëŒ€ê·œëª¨ ë°ì´í„° ì¦ê°•
```bash
# Approach 2 + LoRA í•™ìŠµ
1. ì†ŒëŸ‰ ë°ì´í„°ë¡œ LoRA í•™ìŠµ
2. ì „ì²´ ë°ì´í„°ì— ì ìš©
```
**ì´ìœ :** ë„ë©”ì¸ íŠ¹í™”ë¡œ í’ˆì§ˆ í–¥ìƒ, ëŒ€ê·œëª¨ ì²˜ë¦¬ íš¨ìœ¨ì 

---

## ğŸ“ ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- **Parsing ìƒì„¸:** [parsing/README.md](parsing/README.md)
- **Preprocessing ìƒì„¸:** [preprocessing/README.md](preprocessing/README.md)
- **Inpainting ìƒì„¸ (ëª¨ë¸ í¬í•¨):** [Inpainting/README.md](Inpainting/README.md)
- **Waymo ë³€í™˜ ê°€ì´ë“œ:** [README_WAYMO_CONVERSION.md](README_WAYMO_CONVERSION.md)

---

## ğŸ¤ ê¸°ì—¬ ë° ë¬¸ì˜

- GitHub Issues: https://github.com/DaejunKang/Spatial-AI/issues
- ë¬¸ì„œ ì—…ë°ì´íŠ¸: Pull Request í™˜ì˜

---

**ìµœì¢… ì—…ë°ì´íŠ¸:** 2026-02-09  
**ë²„ì „:** 2.1
