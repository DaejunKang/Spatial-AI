## Inpainting Stage ì¢…í•© ë¶„ì„

**ëª©í‘œ:** ë™ì  ê°ì²´ê°€ ì œê±°ëœ ì •ì  ë°°ê²½ ì´ë¯¸ì§€ ìƒì„±

---

## ğŸ“‹ ë‘ ê°€ì§€ Approach ê°œìš”

### Approach 1: COLMAP-based Scene Reconstruction
**ì „ëµ:** 3D ì¬êµ¬ì„± ê¸°ë°˜ ê³µê°„ì  ì¼ê´€ì„± ìš°ì„ 

### Approach 2: Sequential Multi-Stage Pipeline
**ì „ëµ:** ì‹œê³„ì—´ ì •ë³´ í™œìš© + AI ìƒì„± ê¸°ë°˜ ì ì§„ì  ë³µì›

**ê³µí†µ ìµœì¢… Output:**
```
final_inpainted/
â””â”€â”€ *.jpg  # ë™ì  ê°ì²´ê°€ ì œê±°ëœ ê¹¨ë—í•œ ë°°ê²½ ì´ë¯¸ì§€
```

---

## ğŸ”„ Approach 1: COLMAP-based Scene Reconstruction

### âœ¨ í•µì‹¬ ì•„ì´ë””ì–´
**ì •ì  ì˜ì—­ë§Œìœ¼ë¡œ 3D ì¬êµ¬ì„± â†’ Novel View Synthesisë¡œ êµ¬ë© ì±„ìš°ê¸°**

### ğŸ“¥ Input
| í•­ëª© | í˜•ì‹ | ì¶œì²˜ | ì„¤ëª… |
|-----|------|------|------|
| Images | `images/*.jpg` | Parsing | ì›ë³¸ ì´ë¯¸ì§€ |
| Masks | `masks/{cam}/*.png` | Preprocessing | ë™ì  ê°ì²´ ë§ˆìŠ¤í¬ (0=ë™ì , 255=ì •ì ) |
| Poses | `poses/*.json` | Parsing | ì¹´ë©”ë¼ í¬ì¦ˆ (ì´ˆê¸°ê°’) |

### âš™ï¸ Process

#### **Step 1: Feature Extraction (íŠ¹ì§•ì  ì¶”ì¶œ)**

**ëª©ì :** ì •ì  ì˜ì—­ì—ì„œë§Œ SIFT íŠ¹ì§•ì  ì¶”ì¶œ

**Process:**
```bash
colmap feature_extractor \
    --database_path database.db \
    --image_path images/ \
    --ImageReader.mask_path masks/ \  # ë§ˆìŠ¤í¬ ì ìš©
    --ImageReader.camera_model OPENCV \
    --SiftExtraction.max_num_features 8192
```

**í•µì‹¬ ë¡œì§:**
- ë§ˆìŠ¤í¬ ì ìš©: `mask == 255` ì˜ì—­ì—ì„œë§Œ íŠ¹ì§•ì  ì¶”ì¶œ
- ë™ì  ê°ì²´ëŠ” ì™„ì „íˆ ë¬´ì‹œ â†’ ì •ì  ë°°ê²½ë§Œìœ¼ë¡œ ì¬êµ¬ì„±

**Output:**
- `database.db`: COLMAP ë°ì´í„°ë² ì´ìŠ¤ (íŠ¹ì§•ì , ë””ìŠ¤í¬ë¦½í„° ì €ì¥)

---

#### **Step 2: Feature Matching (íŠ¹ì§•ì  ë§¤ì¹­)**

**ëª©ì :** í”„ë ˆì„ ê°„ ëŒ€ì‘ì  ì°¾ê¸°

**Process:**
```bash
colmap sequential_matcher \
    --database_path database.db \
    --SequentialMatching.overlap 10 \  # ì „í›„ 10 í”„ë ˆì„
    --SequentialMatching.loop_detection 1
```

**í•µì‹¬ ë¡œì§:**
- Sequential matching: ììœ¨ì£¼í–‰ ì‹œí€€ìŠ¤ì— ìµœì í™”
- Loop detection: ë¹„ìŠ·í•œ ì¥ë©´ ì¬ë°©ë¬¸ ê°ì§€

**Output:**
- `database.db` (ì—…ë°ì´íŠ¸): ë§¤ì¹­ ê²°ê³¼ ì¶”ê°€

---

#### **Step 3: Structure from Motion (SfM, 3D ì¬êµ¬ì„±)**

**ëª©ì :** ì¹´ë©”ë¼ í¬ì¦ˆ ì¶”ì • ë° Sparse 3D í¬ì¸íŠ¸ ìƒì„±

**Process:**
```bash
colmap mapper \
    --database_path database.db \
    --image_path images/ \
    --output_path sparse/
```

**í•µì‹¬ ë¡œì§:**
- Incremental SfM:
  1. ì´ˆê¸° ì´ë¯¸ì§€ ìŒ ì„ íƒ
  2. Essential matrix ë¶„í•´ â†’ Pose ì¶”ì •
  3. Triangulation â†’ 3D í¬ì¸íŠ¸ ìƒì„±
  4. Bundle Adjustment â†’ ì „ì—­ ìµœì í™”
  5. ìƒˆ ì´ë¯¸ì§€ ì¶”ê°€ ë°˜ë³µ

**Output:**
```
sparse/0/
â”œâ”€â”€ cameras.bin       # ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„°
â”œâ”€â”€ images.bin        # ì¹´ë©”ë¼ í¬ì¦ˆ
â””â”€â”€ points3D.bin      # 3D í¬ì¸íŠ¸ í´ë¼ìš°ë“œ (ì •ì  ë°°ê²½ë§Œ)
```

**íŠ¹ì§•:**
- ë™ì  ê°ì²´ê°€ ì œê±°ëœ ìˆœìˆ˜ ë°°ê²½ 3D ëª¨ë¸
- Multi-view geometric consistency ë³´ì¥

---

#### **Step 4: Dense Reconstruction (MVS, ë°€ì§‘ ì¬êµ¬ì„±)**

**ëª©ì :** Sparse 3Dë¥¼ Dense Depth Mapìœ¼ë¡œ í™•ì¥

**Process:**
```bash
# 1. Image Undistortion
colmap image_undistorter \
    --input_path sparse/0 \
    --output_path dense/

# 2. Patch Match Stereo
colmap patch_match_stereo \
    --workspace_path dense/

# 3. Stereo Fusion
colmap stereo_fusion \
    --workspace_path dense/ \
    --output_path dense/fused.ply
```

**í•µì‹¬ ë¡œì§:**
1. **Undistortion**: ì™œê³¡ ì œê±°
2. **Patch Match Stereo**: 
   - ê° í”½ì…€ì˜ depth & normal ì¶”ì •
   - Multi-view photo consistency ìµœëŒ€í™”
3. **Fusion**: 
   - ì—¬ëŸ¬ ë·°ì˜ depthë¥¼ í•˜ë‚˜ë¡œ ìœµí•©
   - Outlier filtering

**Output:**
```
dense/
â”œâ”€â”€ images/              # Undistorted images
â”œâ”€â”€ stereo/
â”‚   â”œâ”€â”€ depth_maps/      # ê° ë·°ì˜ depth map (.bin)
â”‚   â”œâ”€â”€ normal_maps/     # Normal map
â”‚   â””â”€â”€ consistency_graphs/
â””â”€â”€ fused.ply            # Dense 3D point cloud (ì •ì  ë°°ê²½)
```

---

#### **Step 5: Hole Filling (êµ¬ë© ì±„ìš°ê¸°)**

**ëª©ì :** ì¬êµ¬ì„±ëœ 3Dë¥¼ ì›ë˜ ë·°ì— ë Œë”ë§í•˜ì—¬ ë™ì  ê°ì²´ ì˜ì—­ ì±„ìš°ê¸°

**í•µì‹¬ ë¡œì§:**
```python
for each frame:
    # 1. Load COLMAP depth map
    depth_map = read_colmap_depth(f"dense/stereo/depth_maps/{frame}.bin")
    
    # 2. Load original image & mask
    image = cv2.imread(f"images/{frame}.jpg")
    mask = cv2.imread(f"masks/{frame}.png")
    hole_mask = (mask == 0)  # ë™ì  ê°ì²´ ì˜ì—­
    
    # 3. Inpaint using depth guidance
    # Option A: Depth-based priority inpainting
    result = depth_guided_inpaint(image, hole_mask, depth_map)
    
    # Option B: Multi-view synthesis
    # ì´ì›ƒ ë·°ë“¤ì˜ ì •ë³´ë¥¼ í˜„ì¬ ë·°ì— íˆ¬ì˜
    result = multi_view_synthesis(image, hole_mask, depth_map, neighbor_views)
    
    # 4. Save
    cv2.imwrite(f"final_inpainted/{frame}.jpg", result)
```

**ë°©ë²•ë¡ :**

**A. Depth-guided Inpainting:**
- ê¹Šì´ ì •ë³´ë¡œ ìš°ì„ ìˆœìœ„ ì„¤ì •
- ê°€ê¹Œìš´ ë°°ê²½ë¶€í„° ì±„ì›Œë‚˜ê°
- OpenCV inpaint + depth priority queue

**B. Multi-view Synthesis:**
- ì´ì›ƒ í”„ë ˆì„ì˜ 3D í¬ì¸íŠ¸ë¥¼ í˜„ì¬ ë·°ì— íˆ¬ì˜
- ê°€ë ¤ì§„(occluded) ë°°ê²½ì„ ë‹¤ë¥¸ ë·°ì—ì„œ ë³µì›
- Weighted blending (confidence based)

**Output:**
- `final_inpainted/*.jpg`: ìµœì¢… ê²°ê³¼ (ì„ì‹œ, Step 6 ì „)

---

#### **Step 6: Post-processing (í›„ì²˜ë¦¬)**

**ëª©ì :** ì‹œê°„ì  ì¼ê´€ì„± ë° í’ˆì§ˆ í–¥ìƒ

**Process:**
```python
# 1. Temporal Smoothing
for i, frame in enumerate(frames):
    # ì´ì›ƒ í”„ë ˆì„ê³¼ ë¸”ë Œë”©
    neighbors = frames[i-1:i+2]  # Â±1 í”„ë ˆì„
    smoothed = weighted_average(frame, neighbors, weights=[0.2, 0.8, 0.2])
    
# 2. Texture Noise (ì„ íƒì )
# ë„ˆë¬´ ë§¤ëˆí•œ ì˜ì—­ì— ë¯¸ì„¸í•œ ë…¸ì´ì¦ˆ ì¶”ê°€
noise = np.random.normal(0, 3, smoothed.shape)
final = smoothed + noise * texture_mask

# 3. Seam Blending
# ì¸í˜ì¸íŒ… ê²½ê³„ë¥¼ ë¶€ë“œëŸ½ê²Œ
final = poisson_blending(final, original, hole_mask)
```

**Output:**
- `final_inpainted/*.jpg`: ìµœì¢… ì™„ì„± ê²°ê³¼

---

### ğŸ“¤ Final Output

```
final_inpainted/
â”œâ”€â”€ frame_000.jpg
â”œâ”€â”€ frame_001.jpg
â””â”€â”€ ...
```

**íŠ¹ì§•:**
- 3D ê¸°í•˜í•™ì  ì¼ê´€ì„± ë³´ì¥
- Multi-view consistency ìë™ ìœ ì§€
- ë°°ê²½ êµ¬ì¡°ê°€ ë³µì¡í•´ë„ ê°•ê±´

---

### ğŸ¯ ì¥ë‹¨ì 

#### âœ… ì¥ì 
1. **Multi-view Consistency**: 3D ì¬êµ¬ì„±ìœ¼ë¡œ ë·° ê°„ ì¼ê´€ì„± ìë™ ë³´ì¥
2. **Geometric Accuracy**: ì •í™•í•œ ë°°ê²½ ê¸°í•˜í•™ ë³µì›
3. **Robustness**: ë³µì¡í•œ ë°°ê²½ êµ¬ì¡°ì—ë„ ê°•ê±´
4. **No Learning Required**: í•™ìŠµ ë°ì´í„° ë¶ˆí•„ìš”

#### âš ï¸ ë‹¨ì 
1. **ê³„ì‚° ë¹„ìš©**: SfM + MVSëŠ” ë§¤ìš° ëŠë¦¼ (~ìˆ˜ ì‹œê°„)
2. **í…ìŠ¤ì²˜ í•œê³„**: Texture-less ì˜ì—­ì—ì„œ ì‹¤íŒ¨ ê°€ëŠ¥
3. **ì™„ì „ì„± ë¬¸ì œ**: ì¬êµ¬ì„± ì‹¤íŒ¨ ì‹œ êµ¬ë© ë‚¨ìŒ
4. **COLMAP ì˜ì¡´ì„±**: ì™¸ë¶€ ë„êµ¬ í•„ìˆ˜

---

### ğŸ“Š ì„±ëŠ¥ ì§€í‘œ

| í•­ëª© | ê°’ | ì„¤ëª… |
|-----|---|------|
| **ì²˜ë¦¬ ì†ë„** | ~1-5 ì‹œê°„ | 100 í”„ë ˆì„ ê¸°ì¤€ (GPU) |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©** | ~8-16GB | Dense MVS ë‹¨ê³„ |
| **3D í¬ì¸íŠ¸ ìˆ˜** | ~1M-10M | Sparse ë‹¨ê³„ |
| **Depth ì •í™•ë„** | ~cm ë‹¨ìœ„ | ì •ì  ì˜ì—­ ê¸°ì¤€ |
| **ì„±ê³µë¥ ** | ~80-90% | ì¬êµ¬ì„± ì„±ê³µë¥  |

---

## ğŸ”„ Approach 2: Sequential Multi-Stage Pipeline

### âœ¨ í•µì‹¬ ì•„ì´ë””ì–´
**ì‹œê³„ì—´ ëˆ„ì  â†’ ê¸°í•˜í•™ì  ê°€ì´ë“œ â†’ AI ìƒì„±ì˜ 3ë‹¨ê³„ ì ì§„ì  ë³µì›**

### ğŸ“¥ Input
| í•­ëª© | í˜•ì‹ | ì¶œì²˜ | ì„¤ëª… |
|-----|------|------|------|
| Images | `images/*.jpg` | Parsing | ì›ë³¸ ì´ë¯¸ì§€ |
| Masks | `masks/{cam}/*.png` | Preprocessing | ë™ì  ê°ì²´ ë§ˆìŠ¤í¬ |
| Poses | `poses/*.json` | Parsing | ì¹´ë©”ë¼ í¬ì¦ˆ |
| Depth Maps | `depth_maps/{cam}/*.png` | Preprocessing | LiDAR ê¹Šì´ ë§µ (ì„ íƒ) |

### âš™ï¸ Process

---

### **Step 1: Temporal Accumulation (ì‹œê³„ì—´ ëˆ„ì )**
(`step1_temporal_accumulation.py`)

#### ëª©ì 
**ì—¬ëŸ¬ í”„ë ˆì„ì˜ ì •ì  ë°°ê²½ì„ 3Dë¡œ ëˆ„ì í•˜ì—¬ ë™ì  ê°ì²´ ë’¤ì˜ ë°°ê²½ ë³µì›**

#### í•µì‹¬ ì•„ì´ë””ì–´
```
ë™ì  ê°ì²´ê°€ ì´ë™ â†’ ì´ì „/ì´í›„ í”„ë ˆì„ì—ì„œëŠ” ê·¸ ìœ„ì¹˜ê°€ ë°°ê²½
â†’ ì‹œê³„ì—´ ì •ë³´ë¥¼ 3Dë¡œ ìœµí•©í•˜ë©´ ì™„ì „í•œ ë°°ê²½ íšë“
```

#### Process

**1.1 Forward Pass (ìˆœë°©í–¥ ëˆ„ì )**
```python
# ê° í”„ë ˆì„ì„ ê¸°ì¤€ìœ¼ë¡œ Forwardë¡œ ëˆ„ì 
for ref_frame_idx in range(len(frames)):
    # 3D í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ì´ˆê¸°í™”
    accumulated_points = PointCloud()
    
    # ì´í›„ í”„ë ˆì„ë“¤ì„ ìƒ˜í”Œë§
    for src_frame_idx in range(ref_frame_idx + 1, len(frames), sample_interval):
        # 1. Source í”„ë ˆì„ì˜ ì •ì  ì˜ì—­ë§Œ ì„ íƒ
        src_image = load_image(src_frame_idx)
        src_mask = load_mask(src_frame_idx)
        static_pixels = src_image[src_mask == 255]
        
        # 2. ê¹Šì´ ì¶”ì • (LiDAR or Pseudo-depth)
        depth = load_depth(src_frame_idx)
        
        # 3. 2D â†’ 3D Unprojection (ì—­íˆ¬ì˜)
        points_3d_src = unproject(static_pixels, depth, K_src, T_src_to_world)
        
        # 4. 3D í¬ì¸íŠ¸ ëˆ„ì 
        accumulated_points.add(points_3d_src)
```

**1.2 Voxel Downsampling (ì¤‘ë³µ ì œê±°)**
```python
    # ì¤‘ë³µ í¬ì¸íŠ¸ ë° ë…¸ì´ì¦ˆ ì œê±°
    accumulated_points = voxel_downsample(
        accumulated_points, 
        voxel_size=0.05  # 5cm ê·¸ë¦¬ë“œ
    )
```

**1.3 Back-projection (ì¬íˆ¬ì˜)**
```python
    # ëˆ„ì ëœ 3D í¬ì¸íŠ¸ë¥¼ ê¸°ì¤€ í”„ë ˆì„ì— íˆ¬ì˜
    # 1. World â†’ Reference Camera ë³€í™˜
    points_3d_ref = T_world_to_ref @ accumulated_points
    
    # 2. 3D â†’ 2D Projection
    pixels_2d, colors = project(
        points_3d_ref, 
        K_ref, 
        image_size=(H, W)
    )
    
    # 3. Warped ì´ë¯¸ì§€ ìƒì„±
    warped_image = np.zeros((H, W, 3), dtype=np.uint8)
    for (x, y), color in zip(pixels_2d, colors):
        warped_image[y, x] = color
    
    # 4. êµ¬ë© ì±„ìš°ê¸° (ì›ë³¸ ì •ì  ì˜ì—­ + Warped ë°°ê²½)
    ref_mask = load_mask(ref_frame_idx)
    final_image = ref_image.copy()
    
    # ë™ì  ê°ì²´ ì˜ì—­ë§Œ Warpedë¡œ êµì²´
    hole_mask = (ref_mask == 0)
    final_image[hole_mask] = warped_image[hole_mask]
    
    # 5. ì €ì¥
    save_image(f"step1_warped/frame_{ref_frame_idx}.png", final_image)
```

#### Input/Output

**Input:**
- `images/*.jpg`: ì›ë³¸ ì´ë¯¸ì§€
- `masks/*.png`: ë™ì  ê°ì²´ ë§ˆìŠ¤í¬
- `poses/*.json`: ì¹´ë©”ë¼ í¬ì¦ˆ
- `depth_maps/*.png` (ì„ íƒ): LiDAR ê¹Šì´

**Output:**
```
step1_warped/
â””â”€â”€ *.png  # ì‹œê³„ì—´ ëˆ„ì ìœ¼ë¡œ êµ¬ë©ì´ ì±„ì›Œì§„ ì´ë¯¸ì§€
```

**íŠ¹ì§•:**
- ì‹¤ì œ ë°°ê²½ ë°ì´í„° ì‚¬ìš© (ìƒì„± ì•„ë‹˜)
- Photo-realistic (ì›ë³¸ í”½ì…€ í™œìš©)
- í•˜ì§€ë§Œ ì™„ì „íˆ ëª» ì±„ìš¸ ìˆ˜ ìˆìŒ (Occlusion)

---

### **Step 2: Geometric Guide Generation (ê¸°í•˜í•™ì  ê°€ì´ë“œ ìƒì„±)**
(`step2_geometric_guide.py`)

#### ëª©ì 
**Step 1ì—ì„œ ëª» ì±„ìš´ êµ¬ë©ì— ëŒ€í•œ ê¸°í•˜í•™ì  íŒíŠ¸ ì œê³µ (Step 3 ControlNet ì…ë ¥)**

#### í•µì‹¬ ì•„ì´ë””ì–´
```
ë„ë¡œ í™˜ê²½ = ì£¼ë¡œ í‰ë©´
â†’ RANSAC í‰ë©´ ì¶”ì •ìœ¼ë¡œ êµ¬ë© ì˜ì—­ì˜ depth ì˜ˆì¸¡
â†’ ControlNetì´ ì´ë¥¼ ê°€ì´ë“œë¡œ ì‚¬ìš©
```

#### Process

**2.1 êµ¬ë© ì˜ì—­ íƒì§€**
```python
for frame_idx in range(len(frames)):
    # 1. Step 1 ê²°ê³¼ ë¡œë“œ
    warped_image = load_image(f"step1_warped/frame_{frame_idx}.png")
    original_mask = load_mask(f"masks/frame_{frame_idx}.png")
    
    # 2. ì—¬ì „íˆ ê²€ì€ìƒ‰ì¸ ê³³ = Step 1 ì‹¤íŒ¨ ì˜ì—­
    still_missing = (np.sum(warped_image, axis=2) == 0)
    
    # 3. ìµœì¢… êµ¬ë© ë§ˆìŠ¤í¬ = ì›ë˜ ë™ì  ì˜ì—­ AND Step 1 ì‹¤íŒ¨
    hole_mask = (original_mask == 0) & still_missing
```

**2.2 LiDAR Depth í™œìš© (if available)**
```python
    if use_lidar:
        # LiDAR depth map ë¡œë“œ
        lidar_depth = load_depth(f"depth_maps/frame_{frame_idx}.png")
        
        # ìœ íš¨í•œ depth ì˜ì—­ì—ì„œ í‰ë©´ ì¶”ì •
        valid_depth_mask = (lidar_depth > 0) & (~hole_mask)
        valid_points = lidar_depth[valid_depth_mask]
```

**2.3 í‰ë©´ ì¶”ì • (RANSAC)**
```python
    # ë°”ë‹¥ ì˜ì—­ ì„ íƒ (ì´ë¯¸ì§€ í•˜ë‹¨ 60%)
    bottom_region = image[int(H * 0.4):, :]
    bottom_depth = lidar_depth[int(H * 0.4):, :]
    
    # 2D ì¢Œí‘œ + Depth â†’ 3D í¬ì¸íŠ¸
    points_3d = []
    for y, x in zip(*np.where(bottom_depth > 0)):
        Z = bottom_depth[y, x]
        X = (x - cx) * Z / fx
        Y = (y - cy) * Z / fy
        points_3d.append([X, Y, Z])
    
    points_3d = np.array(points_3d)
    
    # RANSAC í‰ë©´ í”¼íŒ…
    # í‰ë©´ ë°©ì •ì‹: aX + bY + cZ + d = 0
    plane_model, inliers = fit_plane_ransac(
        points_3d,
        distance_threshold=0.05,  # 5cm
        max_iterations=1000
    )
    
    a, b, c, d = plane_model
```

**2.4 êµ¬ë© ì˜ì—­ Depth ì˜ˆì¸¡**
```python
    # êµ¬ë© ì˜ì—­ì˜ ê° í”½ì…€ì— ëŒ€í•´ í‰ë©´ ë°©ì •ì‹ìœ¼ë¡œ depth ê³„ì‚°
    depth_guide = lidar_depth.copy()
    
    for y, x in zip(*np.where(hole_mask)):
        # í‰ë©´ êµì  ê³„ì‚°
        # Ray: (x-cx)/fx * t, (y-cy)/fy * t, t
        # Plane: a*X + b*Y + c*Z + d = 0
        
        X_dir = (x - cx) / fx
        Y_dir = (y - cy) / fy
        Z_dir = 1.0
        
        # t = -d / (a*X_dir + b*Y_dir + c*Z_dir)
        t = -d / (a * X_dir + b * Y_dir + c * Z_dir)
        
        if t > 0:  # ì¹´ë©”ë¼ ì•
            depth_guide[y, x] = t
```

**2.5 ë³´ê°„ ë° ì •ì œ**
```python
    # ë¶€ë“œëŸ¬ìš´ ì „í™˜ì„ ìœ„í•œ ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬
    kernel_size = 15
    depth_guide = cv2.GaussianBlur(depth_guide, (kernel_size, kernel_size), 0)
    
    # uint16 ë³€í™˜ (mm ë‹¨ìœ„)
    depth_guide_mm = (depth_guide * 1000).astype(np.uint16)
    
    # ì €ì¥
    save_depth(f"step2_depth_guide/frame_{frame_idx}.png", depth_guide_mm)
    save_mask(f"step2_hole_masks/frame_{frame_idx}.png", hole_mask.astype(np.uint8) * 255)
```

#### Input/Output

**Input:**
- `step1_warped/*.png`: Step 1 ê²°ê³¼
- `masks/*.png`: ì›ë³¸ ë™ì  ê°ì²´ ë§ˆìŠ¤í¬
- `depth_maps/*.png` (ì„ íƒ): LiDAR ê¹Šì´
- `poses/*.json`: ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„°

**Output:**
```
step2_depth_guide/
â””â”€â”€ *.png  # êµ¬ë© ì˜ì—­ì˜ depth guide (uint16)

step2_hole_masks/
â””â”€â”€ *.png  # Step 1ì—ì„œ ëª» ì±„ìš´ êµ¬ë© ë§ˆìŠ¤í¬ (uint8)
```

**íŠ¹ì§•:**
- ê¸°í•˜í•™ì ìœ¼ë¡œ ê·¸ëŸ´ë“¯í•œ(plausible) depth
- ControlNetì˜ structure guidanceë¡œ í™œìš©
- í‰ë©´ ê°€ì • (ë„ë¡œì— ì í•©)

---

### **Step 3: Final Inpainting (ìµœì¢… AI ìƒì„±)**
(`step3_final_inpainting.py`)

#### ëª©ì 
**Stable Diffusion + ControlNet + LoRAë¡œ ê³ í’ˆì§ˆ ìµœì¢… ì¸í˜ì¸íŒ…**

#### í•µì‹¬ ì•„ì´ë””ì–´
```
Step 1 (ì‹¤ì œ ë°°ê²½) + Step 2 (ê¸°í•˜í•™ì  ê°€ì´ë“œ) + AI ìƒì„±
â†’ Photo-realistic & Geometrically consistent
```

#### Process

**3.1 ëª¨ë¸ ì´ˆê¸°í™”**
```python
from diffusers import (
    StableDiffusionControlNetInpaintPipeline,
    ControlNetModel,
    UniPCMultistepScheduler
)

# 1. ControlNet ë¡œë“œ (Depth ê°€ì´ë“œ)
controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/control_v11f1p_sd15_depth",
    torch_dtype=torch.float16
)

# 2. Stable Diffusion 1.5 ë¡œë“œ
pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    controlnet=controlnet,
    torch_dtype=torch.float16,
    safety_checker=None
)

# 3. ìŠ¤ì¼€ì¤„ëŸ¬ ìµœì í™” (ì†ë„ 2ë°°)
pipe.scheduler = UniPCMultistepScheduler.from_config(
    pipe.scheduler.config
)

# 4. LoRA ë¡œë“œ (Waymo ì „ìš© í•™ìŠµ ê°€ì¤‘ì¹˜)
if lora_path:
    pipe.load_lora_weights(lora_path)
    trigger_word = "WaymoStyle road"
else:
    trigger_word = "high quality realistic asphalt road"
```

**3.2 í”„ë ˆì„ë³„ ì¸í˜ì¸íŒ…**
```python
for frame_idx in range(len(frames)):
    # 1. Load Inputs
    warped_img = load_image(f"step1_warped/frame_{frame_idx}.png")
    depth_guide = load_depth(f"step2_depth_guide/frame_{frame_idx}.png")
    hole_mask = load_mask(f"step2_hole_masks/frame_{frame_idx}.png")
    original_img = load_image(f"images/frame_{frame_idx}.jpg")
    original_mask = load_mask(f"masks/frame_{frame_idx}.png")
    
    # 2. Base Image ìƒì„±
    # ì›ë³¸ + Step 1 ê²°ê³¼ í•©ì„±
    base_image = original_img.copy()
    valid_warped = (np.sum(warped_img, axis=2) > 0)  # Warpedê°€ ìœ íš¨í•œ ê³³
    mask_bool = (original_mask == 0)  # ë™ì  ê°ì²´ ì˜ì—­
    base_image[mask_bool & valid_warped] = warped_img[mask_bool & valid_warped]
    
    # 3. ìµœì¢… ë§ˆìŠ¤í¬: Step 1ë¡œë„ ëª» ì±„ìš´ ì§„ì§œ êµ¬ë©
    final_mask = cv2.bitwise_and(original_mask == 0, hole_mask > 0)
```

**3.3 Preprocessing (Numpy â†’ PIL)**
```python
    # PIL ë³€í™˜
    img_pil = Image.fromarray(cv2.cvtColor(base_image, cv2.COLOR_BGR2RGB))
    mask_pil = Image.fromarray(final_mask.astype(np.uint8) * 255)
    
    # Depth Normalization (ControlNet ì…ë ¥ í˜•ì‹)
    depth_norm = (depth_guide / 65535.0 * 255).astype(np.uint8)
    depth_pil = Image.fromarray(np.stack([depth_norm]*3, axis=-1))  # 3ì±„ë„
```

**3.4 Prompt Engineering**
```python
    # ììœ¨ì£¼í–‰ ë„ë¡œì— íŠ¹í™”ëœ í”„ë¡¬í”„íŠ¸
    positive_prompt = (
        f"{trigger_word}, sharp focus, photorealistic, 8k uhd, "
        f"detailed pavement texture, driving scene, clear lane markings"
    )
    
    negative_prompt = (
        "blur, low quality, artifacts, watermark, text, "
        "cars, pedestrians, objects, obstacles, distortions"
    )
```

**3.5 Inference (Diffusion)**
```python
    with torch.inference_mode():
        result_pil = pipe(
            prompt=positive_prompt,
            negative_prompt=negative_prompt,
            image=img_pil,
            mask_image=mask_pil,
            control_image=depth_pil,  # ControlNet depth guide
            num_inference_steps=20,    # UniPC: 20 stepìœ¼ë¡œ ì¶©ë¶„
            guidance_scale=7.5,
            controlnet_conditioning_scale=0.8,  # Depth ì˜í–¥ë„
            strength=1.0  # ë§ˆìŠ¤í¬ ì˜ì—­ ì™„ì „ ì¬ìƒì„±
        ).images[0]
```

**3.6 Post-processing (PIL â†’ Numpy)**
```python
    result_np = cv2.cvtColor(np.array(result_pil), cv2.COLOR_RGB2BGR)
    
    # ì›ë³¸ ë³´ì¡´ (ë§ˆìŠ¤í¬ ë°”ê¹¥ì€ ì›ë³¸ ìœ ì§€)
    final_image = base_image.copy()
    final_image[final_mask > 0] = result_np[final_mask > 0]
    
    # ì €ì¥
    save_image(f"step3_final_inpainted/frame_{frame_idx}.jpg", final_image)
```

**3.7 Finalization**
```python
# step3_final_inpainted/ â†’ final_inpainted/ ë³µì‚¬
import shutil
for file in Path("step3_final_inpainted").glob("*.jpg"):
    shutil.copy(file, f"final_inpainted/{file.name}")
```

#### Input/Output

**Input:**
- `step1_warped/*.png`: Step 1 ì‹œê³„ì—´ ëˆ„ì  ê²°ê³¼
- `step2_depth_guide/*.png`: Step 2 ê¹Šì´ ê°€ì´ë“œ
- `step2_hole_masks/*.png`: ìµœì¢… êµ¬ë© ë§ˆìŠ¤í¬
- `images/*.jpg`: ì›ë³¸ ì´ë¯¸ì§€ (ì°¸ì¡°ìš©)
- `masks/*.png`: ì›ë³¸ ë™ì  ê°ì²´ ë§ˆìŠ¤í¬

**Output:**
```
step3_final_inpainted/
â””â”€â”€ *.jpg  # AI ìƒì„± ìµœì¢… ê²°ê³¼

final_inpainted/  # Approach 1ê³¼ ë™ì¼ ê²½ë¡œ
â””â”€â”€ *.jpg  # ìµœì¢… ì¶œë ¥ (ë³µì‚¬ë³¸)
```

**íŠ¹ì§•:**
- Stable Diffusionì˜ ìƒì„± ëŠ¥ë ¥ í™œìš©
- ControlNetìœ¼ë¡œ ê¸°í•˜í•™ì  ì œì•½
- LoRAë¡œ Waymo ë„ë©”ì¸ íŠ¹í™”

---

### ğŸ“¤ Final Output

```
final_inpainted/
â”œâ”€â”€ frame_000.jpg
â”œâ”€â”€ frame_001.jpg
â””â”€â”€ ...
```

**ìƒì„± ê³¼ì •:**
1. **Step 1**: ì‹¤ì œ ë°°ê²½ í”½ì…€ ìµœëŒ€í•œ í™œìš© (Photo-realistic)
2. **Step 2**: ë‚¨ì€ êµ¬ë©ì— ê¸°í•˜í•™ì  íŒíŠ¸
3. **Step 3**: AIê°€ íŒíŠ¸ ê¸°ë°˜ìœ¼ë¡œ ê³ í’ˆì§ˆ ìƒì„±

---

### ğŸ¯ ì¥ë‹¨ì 

#### âœ… ì¥ì 
1. **ì†ë„**: COLMAPë³´ë‹¤ 5-10ë°° ë¹ ë¦„ (~10-30ë¶„)
2. **í’ˆì§ˆ**: AI ìƒì„±ìœ¼ë¡œ í…ìŠ¤ì²˜ í’ë¶€
3. **ì™„ì „ì„±**: í•­ìƒ ëª¨ë“  êµ¬ë© ì±„ì›€ (ìƒì„±ì´ë¯€ë¡œ)
4. **ìœ ì—°ì„±**: LoRAë¡œ ë„ë©”ì¸ íŠ¹í™” ê°€ëŠ¥
5. **ì‹¤ì œ ë°ì´í„° ìš°ì„ **: Step 1ì—ì„œ ì›ë³¸ í™œìš©

#### âš ï¸ ë‹¨ì 
1. **í•™ìŠµ ì˜ì¡´**: Stable Diffusion ëª¨ë¸ í•„ìš” (~4GB)
2. **GPU í•„ìˆ˜**: Inferenceì— GPU í•„ìš”
3. **Multi-view Consistency**: ë³´ì¥ ì•ˆ ë¨ (í”„ë ˆì„ë³„ ë…ë¦½ ìƒì„±)
4. **Hallucination**: AIê°€ ì˜ëª»ëœ íŒ¨í„´ ìƒì„± ê°€ëŠ¥

---

### ğŸ“Š ì„±ëŠ¥ ì§€í‘œ

| í•­ëª© | ê°’ | ì„¤ëª… |
|-----|---|------|
| **ì²˜ë¦¬ ì†ë„** | ~10-30ë¶„ | 100 í”„ë ˆì„ ê¸°ì¤€ (GPU) |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©** | ~6GB VRAM | Stable Diffusion 1.5 |
| **Step 1 ë³µì›ë¥ ** | ~70-85% | ì‹œê³„ì—´ë¡œ ì±„ìš¸ ìˆ˜ ìˆëŠ” ë¹„ìœ¨ |
| **Step 3 í’ˆì§ˆ** | LPIPS ~0.1 | ì›ë³¸ ëŒ€ë¹„ perceptual distance |
| **ì™„ì „ì„±** | 100% | ëª¨ë“  êµ¬ë© ì±„ì›€ ë³´ì¥ |

---

## ğŸ“Š ë‘ Approach ë¹„êµ

| í•­ëª© | Approach 1 (COLMAP) | Approach 2 (Sequential) |
|-----|---------------------|------------------------|
| **í•µì‹¬ ì „ëµ** | 3D ì¬êµ¬ì„± ê¸°ë°˜ | ì‹œê³„ì—´ + AI ìƒì„± |
| **ì²˜ë¦¬ ì†ë„** | ëŠë¦¼ (~1-5ì‹œê°„) | ë¹ ë¦„ (~10-30ë¶„) |
| **Multi-view Consistency** | âœ… ìë™ ë³´ì¥ | âš ï¸ ë³´ì¥ ì•ˆ ë¨ |
| **í…ìŠ¤ì²˜ í’ˆì§ˆ** | ë³´í†µ (ì›ë³¸ ì¬ì‚¬ìš©) | ë†’ìŒ (AI ìƒì„±) |
| **ì™„ì „ì„±** | âš ï¸ ì¬êµ¬ì„± ì‹¤íŒ¨ ì‹œ êµ¬ë© | âœ… 100% ì±„ì›€ |
| **GPU ìš”êµ¬** | ì„ íƒì  | í•„ìˆ˜ (Step 3) |
| **ì™¸ë¶€ ì˜ì¡´ì„±** | COLMAP í•„ìˆ˜ | PyTorch, Diffusers |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©** | ë†’ìŒ (8-16GB) | ì¤‘ê°„ (6GB VRAM) |
| **ë„ë©”ì¸ íŠ¹í™”** | ë¶ˆê°€ëŠ¥ | âœ… LoRAë¡œ ê°€ëŠ¥ |
| **ê¸°í•˜í•™ì  ì •í™•ë„** | ë§¤ìš° ë†’ìŒ | ë†’ìŒ (ControlNet) |
| **ì ìš© í™˜ê²½** | êµ¬ì¡°ì  ì¬êµ¬ì„± ì¤‘ìš” | ë¹ ë¥¸ ì²˜ë¦¬ í•„ìš” |

---

## ğŸ”„ í†µí•© ë°ì´í„° í”Œë¡œìš°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Preprocessing Stage Output              â”‚
â”‚  - images/                                  â”‚
â”‚  - masks/ (ë™ì  ê°ì²´ ë§ˆìŠ¤í¬)                 â”‚
â”‚  - poses/                                   â”‚
â”‚  - depth_maps/ (LiDAR)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
        â”‚             â”‚
        â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Approach 1   â”‚  â”‚ Approach 2               â”‚
â”‚ COLMAP-based â”‚  â”‚ Sequential Pipeline      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                 â”‚
       â”‚                 â”œâ”€â–º Step 1: Temporal Accumulation
       â”‚                 â”‚    â†’ step1_warped/
       â”‚                 â”‚
       â”œâ”€â–º Feature       â”œâ”€â–º Step 2: Geometric Guide
       â”‚   Extraction    â”‚    â†’ step2_depth_guide/
       â”‚                 â”‚    â†’ step2_hole_masks/
       â”œâ”€â–º SfM           â”‚
       â”‚   (sparse/)     â”œâ”€â–º Step 3: AI Inpainting
       â”‚                 â”‚    â†’ step3_final_inpainted/
       â”œâ”€â–º MVS           â”‚
       â”‚   (dense/)      â”‚
       â”‚                 â”‚
       â”œâ”€â–º Hole Filling  â”‚
       â”‚                 â”‚
       â”œâ”€â–º Post-process  â”œâ”€â–º Copy to final output
       â”‚                 â”‚
       â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         final_inpainted/                     â”‚
â”‚  ë™ì  ê°ì²´ê°€ ì œê±°ëœ ìµœì¢… ë°°ê²½ ì´ë¯¸ì§€           â”‚
â”‚  (ë‘ Approach ëª¨ë‘ ë™ì¼ ê²½ë¡œì— ì¶œë ¥)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ ê° Approachì˜ ì í•©í•œ ì‚¬ìš© ì‚¬ë¡€

### Approach 1 (COLMAP) ì¶”ì²œ ì‹œë‚˜ë¦¬ì˜¤

âœ… **ë‹¤ìŒ ê²½ìš°ì— ì‚¬ìš©:**
1. **3D ì¬êµ¬ì„±ì´ ìµœì¢… ëª©í‘œ**ì¸ ê²½ìš° (NeRF, 3DGS í•™ìŠµìš©)
2. **Multi-view consistencyê°€ í•„ìˆ˜**ì¸ ê²½ìš°
3. **ì‹œê°„ ì œì•½ì´ ì—†ëŠ”** ê²½ìš° (ë°°ì¹˜ ì²˜ë¦¬)
4. **ê¸°í•˜í•™ì  ì •í™•ë„ê°€ ë§¤ìš° ì¤‘ìš”**í•œ ê²½ìš°
5. ì •ì  ì˜ì—­ì´ **ì¶©ë¶„íˆ ë§ì•„** SfMì´ ì„±ê³µí•  ìˆ˜ ìˆëŠ” ê²½ìš°

**ì˜ˆì‹œ:**
- ììœ¨ì£¼í–‰ ì‹œë®¬ë ˆì´í„°ìš© 3D í™˜ê²½ ìƒì„±
- Novel View Synthesis ì—°êµ¬
- ë””ì§€í„¸ íŠ¸ìœˆ êµ¬ì¶•

---

### Approach 2 (Sequential) ì¶”ì²œ ì‹œë‚˜ë¦¬ì˜¤

âœ… **ë‹¤ìŒ ê²½ìš°ì— ì‚¬ìš©:**
1. **ë¹ ë¥¸ ì²˜ë¦¬ê°€ í•„ìš”**í•œ ê²½ìš° (ì‹¤ì‹œê°„ì— ê°€ê¹Œìš´)
2. **2D ì´ë¯¸ì§€ë§Œ í•„ìš”**í•œ ê²½ìš° (3D ë¶ˆí•„ìš”)
3. **GPU ì‚¬ìš© ê°€ëŠ¥**í•œ ê²½ìš°
4. **í…ìŠ¤ì²˜ í’ˆì§ˆì´ ì¤‘ìš”**í•œ ê²½ìš°
5. **ë„ë©”ì¸ íŠ¹í™”** (LoRA í™œìš©) ì›í•˜ëŠ” ê²½ìš°

**ì˜ˆì‹œ:**
- ììœ¨ì£¼í–‰ ë°ì´í„° ì¦ê°• (Data Augmentation)
- ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ì •ì œ
- Inpainting ë²¤ì¹˜ë§ˆí¬ ìƒì„±
- NeRF í•™ìŠµìš© ì „ì²˜ë¦¬ (ë°°ê²½ë§Œ ë¶„ë¦¬)

---

## ğŸš€ ì‹¤í–‰ ì˜ˆì‹œ

### Approach 1: COLMAP-based
```bash
# 1. COLMAP ì„¤ì¹˜ í™•ì¸
colmap --help

# 2. ì‹¤í–‰
python Inpainting/approach1_colmap.py \
    /path/to/waymo_nre_data \
    --colmap_path colmap

# 3. ì¶œë ¥ í™•ì¸
ls /path/to/waymo_nre_data/final_inpainted/
```

---

### Approach 2: Sequential
```bash
# 1. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ)
python -c "from diffusers import StableDiffusionControlNetInpaintPipeline; \
           StableDiffusionControlNetInpaintPipeline.from_pretrained('runwayml/stable-diffusion-v1-5')"

# 2. ì‹¤í–‰ (ê¸°ë³¸ ì„¤ì •)
python Inpainting/approach2_sequential.py \
    /path/to/waymo_nre_data

# 3. ì‹¤í–‰ (ê³ ê¸‰ ì„¤ì •)
python Inpainting/approach2_sequential.py \
    /path/to/waymo_nre_data \
    --voxel_size 0.03 \
    --sample_interval 3 \
    --ground_ratio 0.65 \
    --lora_path ./trained_lora/waymo_road.safetensors

# 4. ì¶œë ¥ í™•ì¸
ls /path/to/waymo_nre_data/final_inpainted/
```

---

### ë‹¨ê³„ë³„ ì‹¤í–‰ (Approach 2)
```bash
# Step 1ë§Œ
python Inpainting/step1_temporal_accumulation.py \
    --data_root /path/to/data

# Step 2ë§Œ (Step 1 ì´í›„)
python Inpainting/step2_geometric_guide.py \
    --data_root /path/to/data

# Step 3ë§Œ (Step 1, 2 ì´í›„)
python Inpainting/step3_final_inpainting.py \
    --data_root /path/to/data \
    --lora_path ./lora_weights.safetensors
```

---

## ğŸ“ ì¶”ê°€ ê°œì„  ì‚¬í•­

### Approach 1
1. **Poisson Reconstruction**: Dense MVS ëŒ€ì‹  Poisson surface reconstruction
2. **Neural Rendering**: NeRF/3DGSë¡œ Novel View Synthesis
3. **Global Optimization**: Bundle Adjustment ê°•í™”

### Approach 2
1. **Multi-view Consistency Loss**: Step 3ì—ì„œ ì´ì›ƒ í”„ë ˆì„ ê³ ë ¤
2. **Diffusion Distillation**: Stable Diffusion ê²½ëŸ‰í™” (ì†ë„ â†‘)
3. **Adaptive Scheduling**: êµ¬ë© í¬ê¸°ì— ë”°ë¼ inference steps ì¡°ì ˆ
4. **Video Diffusion**: Temporal consistencyë¥¼ ìœ„í•œ ë¹„ë””ì˜¤ ëª¨ë¸ í™œìš©

---

## âœ… ìš”êµ¬ì‚¬í•­ ì¶©ì¡± í™•ì¸

| ìš”êµ¬ì‚¬í•­ | Approach 1 | Approach 2 | ìƒíƒœ |
|---------|-----------|-----------|------|
| **ë™ì  ê°ì²´ ì œê±°** | âœ… | âœ… | ì™„ë£Œ |
| **ìµœì¢… Output ë™ì¼** | âœ… `final_inpainted/` | âœ… `final_inpainted/` | ì™„ë£Œ |
| **Input ëª…ì„¸** | âœ… ë¬¸ì„œí™” | âœ… ë¬¸ì„œí™” | ì™„ë£Œ |
| **í”„ë¡œì„¸ìŠ¤ ìƒì„¸** | âœ… 6ë‹¨ê³„ | âœ… 3ë‹¨ê³„ | ì™„ë£Œ |
| **ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸** | âœ… `approach1_colmap.py` | âœ… `approach2_sequential.py` | ì™„ë£Œ |
| **ì„±ëŠ¥ ë¹„êµ** | âœ… í‘œ ì‘ì„± | âœ… í‘œ ì‘ì„± | ì™„ë£Œ |

---

**ìµœì¢… í™•ì¸ì¼**: 2026-02-05  
**ì‘ì„±ì**: Cloud Agent  
**ë²„ì „**: 1.0
